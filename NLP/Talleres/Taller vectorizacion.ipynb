{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwaCEJNvXYua",
        "colab_type": "text"
      },
      "source": [
        "## Asignatura Procesamiento de Lenguaje Natural\n",
        "### Por Elizabeth León Guzmán\n",
        "------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2luOv553tycv",
        "colab_type": "text"
      },
      "source": [
        "# Minería de Texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m2jTXHkItEb",
        "colab_type": "text"
      },
      "source": [
        "En muchas ocasiones es necesario construir modelos que clasifiquen automáticamente un texto dado. Para esto, en algunas oportunidades disponemos de texto anotado, es decir, documentos que fueron previamente clasificados dentro de dos o más categorías. Este tipo de información es de gran utilidad al permitir desarrollar modelos predictivos en una gran variedad de aplicaciones, por ejemplo: \n",
        "\n",
        "* Clasificación de spam en correos.\n",
        "* Detección de depresión en redes sociales.\n",
        "* Análisis de sentimientos.\n",
        "* Clasificación del lenguaje de un texto.\n",
        "* Identificación de noticias falsas.\n",
        "* Atribución de autoría."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiVLhL4aK02-",
        "colab_type": "text"
      },
      "source": [
        "![classtexto](https://miro.medium.com/max/1400/0*6nPN6naiH7lApcvg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-Cmu-QbK4gy",
        "colab_type": "text"
      },
      "source": [
        "La tarea de **clasificación de texto** es una aplicación típica de las técnicas de machine learning de aprendizaje supervisado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "30SRDeOBTHjW"
      },
      "source": [
        "---\n",
        "## 1. Preprocesamiento\n",
        "\n",
        "En este caso, utilizaremos NLTK para el preprocesamiento de los textos. Se define una función para eliminar caracteres especiales, stopwords, mayúsculas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxQSrcDrsJlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importamos las librerías básicas:\n",
        "#   numpy: librería numérica de Python\n",
        "#   pandas: análisis de datos\n",
        "#   re: expresiones regulares\n",
        "#   nltk: procesamiento de LN\n",
        "#   matplotlib: visualizaciones \n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "plt.style.use(\"ggplot\")\n",
        "pd.options.display.max_colwidth = 200\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWSEFW0HsJml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Descargamos recursos en inglés populares: conjuntos de datos, diccionarios, etc (wordnet, snowball, etc. )\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading collection 'popular'\n[nltk_data]    | \n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package omw to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /Users/juan.gama/nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection popular\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix4ghLJMsJnS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definimos un tokenizer que separa texto de signos de puntuación y números.\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "# Descargamos las stopwords para inglés\n",
        "stop_words = nltk.corpus.stopwords.words('english')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QKqDXtm_YbV_",
        "colab": {}
      },
      "source": [
        "# Definimos la función de preprocesamiento\n",
        "def normalize_document(doc):\n",
        "    # Se eliminan caracteres especiales\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.IGNORECASE|re.ASCII)\n",
        "          # Más info compilation flags: https://docs.python.org/3/howto/regex.html#compilation-flags\n",
        "    # Se convierten los textos a minúsculas\n",
        "    doc = doc.lower()\n",
        "    # Se elimin espacios al inicio y final del texto\n",
        "    doc = doc.strip()  \n",
        "    # Tokenizado de documento\n",
        "    tokens = wpt.tokenize(doc)\n",
        "    # Eliminación de stopwords\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Retornamos una versión filtrada del texto\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "# Vectorización de la función\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "# Más info: https://docs.scipy.org/doc/numpy/reference/generated/numpy.vectorize.html\n",
        "# The vectorized function evaluates \"normalize_document\" over successive tuples \n",
        "# of the input arrays like the python map function"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCjecDEgsJoQ",
        "colab_type": "text"
      },
      "source": [
        "Ahora, se mostrará un ejemplo con un dataset sencillo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xg9BoGUsY5qC",
        "outputId": "32d70a00-b0e8-4476-92a0-db04fa02f3e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "# Se define un corpus de ejemplo\n",
        "corpus = ['The sky is blue and beautiful.',\n",
        "'Love this blue and beautiful sky!',\n",
        "'The quick brown fox jumps over the lazy dog.',\n",
        "\"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "'I love green eggs, ham, sausages and bacon!',\n",
        "'The brown fox is quick and the blue dog is lazy!',\n",
        "'The sky is very blue and the sky is very beautiful today',\n",
        "'The dog is lazy but the brown fox is quick!'\n",
        "]\n",
        "# Se asigna una etiqueta a cada texto\n",
        "labels = ['weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather', 'animals']\n",
        "# Creamos un dataframe de pandas con el corpus y las etiquetas\n",
        "corpus_df = pd.DataFrame({\"Document\": corpus, \"Category\": labels})\n",
        "corpus_df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                             Document Category\n0                                      The sky is blue and beautiful.  weather\n1                                   Love this blue and beautiful sky!  weather\n2                        The quick brown fox jumps over the lazy dog.  animals\n3  A king's breakfast has sausages, ham, bacon, eggs, toast and beans     food\n4                         I love green eggs, ham, sausages and bacon!     food\n5                    The brown fox is quick and the blue dog is lazy!  animals\n6            The sky is very blue and the sky is very beautiful today  weather\n7                         The dog is lazy but the brown fox is quick!  animals",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Document</th>\n      <th>Category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The sky is blue and beautiful.</td>\n      <td>weather</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Love this blue and beautiful sky!</td>\n      <td>weather</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>The quick brown fox jumps over the lazy dog.</td>\n      <td>animals</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A king's breakfast has sausages, ham, bacon, eggs, toast and beans</td>\n      <td>food</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I love green eggs, ham, sausages and bacon!</td>\n      <td>food</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>The brown fox is quick and the blue dog is lazy!</td>\n      <td>animals</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>The sky is very blue and the sky is very beautiful today</td>\n      <td>weather</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>The dog is lazy but the brown fox is quick!</td>\n      <td>animals</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43fgOjkGsJo7",
        "colab_type": "code",
        "outputId": "3390745b-fc0c-4430-cb5d-dd69efa6e632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "norm_corpus=normalize_corpus(corpus)\n",
        "norm_corpus"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "array(['sky blue beautiful', 'love blue beautiful sky',\n       'quick brown fox jumps lazy dog',\n       'kings breakfast sausages ham bacon eggs toast beans',\n       'love green eggs ham sausages bacon',\n       'brown fox quick blue dog lazy', 'sky blue sky beautiful today',\n       'dog lazy brown fox quick'], dtype='<U51')"
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9Xszho9-tmp0"
      },
      "source": [
        "---\n",
        "## 2. Representaciones de texto"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dif9ocaNjSX",
        "colab_type": "text"
      },
      "source": [
        "En este notebook nos enfocaremos en algunas técnicas para la **representación de textos** que son comunmente generadas después de tareas de procesamiento de lenguaje natural. En particular, estudiaremos las siguientes representaciones:\n",
        "\n",
        "* Bolsa de palabras (Bag of Words - BoW)\n",
        "* Métodos de pesado como tf-idf\n",
        "* N-grams\n",
        "* Word2Vec\n",
        "* FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-fF-0xtjRDwa"
      },
      "source": [
        "### 2.1 Bolsa de Palabras - Bag of Words (BoW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjzfYiNOWV4Y",
        "colab_type": "text"
      },
      "source": [
        "Se trata de uno de los métodos de representación más simples y consiste en encontrar una distribución de todos los términos $T=\\{t_1,t_2,\\dots,t_l\\}$ que aparecen en un conjunto de documentos $D=\\{d_1,d_2,\\dots,d_m\\}$ por medio de técnicas de conteo, así: $P(T=t_i,D=d_j)$\n",
        "\n",
        "![bow](https://qph.fs.quoracdn.net/main-qimg-4934f0958e121d33717f848230ef664a)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz8YZKegWYi_",
        "colab_type": "text"
      },
      "source": [
        "El enfoque típico para estimar una bolsa de palabras está basado en conteos, más específicamente, se encuentra una matríz:\n",
        "\n",
        " $\\mathbf{Tf}(t_i,d_j)$ \n",
        " \n",
        " que contiene el número de veces que se encuentra un término $t_i$ en un documento $d_j$. Al normalizar esta matriz para que la suma de las ocurrencias de todos los términos en un documento $d_j$ sea uno, se obtiene una estimación de la distribución $P(T=t_i|D=d_j)$. \n",
        "\n",
        "Más info: https://en.wikipedia.org/wiki/Bag-of-words_model\n",
        "\n",
        "Ahora, veamos cómo construir una bolsa de palabras. Primero, lo haremos paso a paso (para propósitos de demostración únicamente). Segundo, usaremos librerías para hacerlo de forma óptima (así lo usaremos normalmente):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Asd1AYoVZXzi",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1.1 Construcción BoW: paso a paso\n",
        "\n",
        "**Para propósitositos didácticos únicamente.** \n",
        "En esta sección usaremos Python básico para construir un sistema NLP rudimentario. Crearemos un vocabulario a partir de todas las palabras de nuestro corpus y luego usaremos la técnica de Bolsa de palabras (BoW) para extraer características de cada documento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUwVBs9Y7POF",
        "colab_type": "text"
      },
      "source": [
        "1. **Construimos el vocabulario**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGWTGVWzbwme",
        "colab_type": "code",
        "outputId": "995bd03a-9a31-40bb-b9d1-5b4fcda3b70e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "print(norm_corpus)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['sky blue beautiful' 'love blue beautiful sky'\n 'quick brown fox jumps lazy dog'\n 'kings breakfast sausages ham bacon eggs toast beans'\n 'love green eggs ham sausages bacon' 'brown fox quick blue dog lazy'\n 'sky blue sky beautiful today' 'dog lazy brown fox quick']\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'breakfast': 0, 'beautiful': 1, 'lazy': 2, 'love': 3, 'dog': 4, 'sausages': 5, 'fox': 6, 'green': 7, 'beans': 8, 'quick': 9, 'today': 10, 'eggs': 11, 'jumps': 12, 'toast': 13, 'brown': 14, 'bacon': 15, 'blue': 16, 'kings': 17, 'ham': 18, 'sky': 19}\n"
        }
      ],
      "source": [
        "vocab = set(' '.join(norm_corpus).split(' '))\n",
        "vocab = { word: idx for idx, word in enumerate(vocab)}\n",
        "print(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-an0Ga37POX",
        "colab_type": "text"
      },
      "source": [
        "2. **Extración de características**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRUgegzL7POZ",
        "colab_type": "code",
        "outputId": "e2948a8e-442f-4925-e298-88a2ca42b45d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "# Creamos una lista vacía para cada documento con una posición para cada palabra en el vocabulario\n",
        "shape = (len(norm_corpus), len(vocab)) \n",
        "tf = np.zeros(shape, np.int32)\n",
        "print(tf)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g7TarJifJik",
        "colab_type": "code",
        "outputId": "4a8fd73f-e6b2-4e50-b9ad-a43af556ee55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf[0][6]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0"
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cvao5WWB7POl",
        "colab_type": "code",
        "outputId": "dfe597bf-fd7c-43c4-c318-b95ef196190a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "# Mapeamos las frecuencias de cada palabra en tf:\n",
        "i = 0\n",
        "for doc in norm_corpus:\n",
        "  for word in doc.split():\n",
        "    tf[i][vocab[word]] += 1\n",
        "  i += 1\n",
        "\n",
        "print(tf)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1]\n [0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1]\n [0 0 1 0 1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0]\n [1 0 0 0 0 1 0 0 1 0 0 1 0 1 0 1 0 1 1 0]\n [0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 1 0 0 1 0]\n [0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 1 0 0 0]\n [0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 2]\n [0 0 1 0 1 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0]]\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9WyC48h7PO7",
        "colab_type": "text"
      },
      "source": [
        "Por ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOoqQW40f1PC",
        "colab_type": "code",
        "outputId": "1ffcc668-31a8-43c8-bbc7-b97ea62a8f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(norm_corpus[0], tf[0])"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "sky blue beautiful [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1]\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkbnHkhugBm7",
        "colab_type": "code",
        "outputId": "884b913f-1d8a-463a-c933-efd14a657995",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(norm_corpus[6])\n",
        "print(tf[6])"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "sky blue sky beautiful today\n[0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 2]\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TvlryEA7PPE",
        "colab_type": "text"
      },
      "source": [
        "Hay que tener en cuenta que si nuestros documentos fueran consideramente más largos y estuviéramos analizando un conjunto mayor de documentos, solamente algunos elementos en la matriz tendrían valores. El vocabulario sería muchísimo más grande y las listas contendrían principalmente valores en cero. Serían **matrices dispersas** con muchas dimensiones (*sparse*)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FixCPsU7PPQ",
        "colab_type": "text"
      },
      "source": [
        "#### 2.1.2 Construcción BoW: usando scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tltpQ3ZnfPN9",
        "colab": {}
      },
      "source": [
        "# Importamos el extractor de características BoW de Scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Chz7YaVZFp7A",
        "outputId": "43422f31-6035-460f-98e9-dfe4d90f4075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Instanciamos el extractor de características para la ocurrencia de palabras\n",
        "cv = CountVectorizer()\n",
        "# Extraemos las características del corpus \n",
        "cv_matrix = cv.fit_transform(norm_corpus)\n",
        "# El resultado es una matriz sparse\n",
        "cv_matrix"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<8x20 sparse matrix of type '<class 'numpy.int64'>'\n\twith 42 stored elements in Compressed Sparse Row format>"
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XwwF1f6nk0X4",
        "outputId": "e9576637-b5ae-4aa9-f141-43e46a3a5ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        }
      },
      "source": [
        "# Convertimos de la representación dispersa (sparse) a la representación densa \n",
        "# para visualizarla como numpy array\n",
        "cv_matrix = cv_matrix.toarray()\n",
        "cv_matrix"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0],\n       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0],\n       [1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0],\n       [1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n       [0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1],\n       [0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0]])"
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1wVcSqyinM0S"
      },
      "source": [
        "Como podemos observar cada documento es representado como una fila y cada columna representa una palabra en el corpus. En este caso los términos del vocabulario están ordenados en orden alfabético. Podemos visualizarlo mejor usando un pandas dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SNDRhqOsji5i",
        "outputId": "8f9f68d0-940f-4db8-a7d9-acaf583c8b77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "# Obtenemos todas las palabras diferentes en el corpus\n",
        "vocab = cv.get_feature_names()\n",
        "# Mostramos el documento y las features\n",
        "tf_df = pd.DataFrame(cv_matrix, columns=vocab)\n",
        "tf_df"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   bacon  beans  beautiful  blue  breakfast  brown  dog  eggs  fox  green  \\\n0      0      0          1     1          0      0    0     0    0      0   \n1      0      0          1     1          0      0    0     0    0      0   \n2      0      0          0     0          0      1    1     0    1      0   \n3      1      1          0     0          1      0    0     1    0      0   \n4      1      0          0     0          0      0    0     1    0      1   \n5      0      0          0     1          0      1    1     0    1      0   \n6      0      0          1     1          0      0    0     0    0      0   \n7      0      0          0     0          0      1    1     0    1      0   \n\n   ham  jumps  kings  lazy  love  quick  sausages  sky  toast  today  \n0    0      0      0     0     0      0         0    1      0      0  \n1    0      0      0     0     1      0         0    1      0      0  \n2    0      1      0     1     0      1         0    0      0      0  \n3    1      0      1     0     0      0         1    0      1      0  \n4    1      0      0     0     1      0         1    0      0      0  \n5    0      0      0     1     0      1         0    0      0      0  \n6    0      0      0     0     0      0         0    2      0      1  \n7    0      0      0     1     0      1         0    0      0      0  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bacon</th>\n      <th>beans</th>\n      <th>beautiful</th>\n      <th>blue</th>\n      <th>breakfast</th>\n      <th>brown</th>\n      <th>dog</th>\n      <th>eggs</th>\n      <th>fox</th>\n      <th>green</th>\n      <th>ham</th>\n      <th>jumps</th>\n      <th>kings</th>\n      <th>lazy</th>\n      <th>love</th>\n      <th>quick</th>\n      <th>sausages</th>\n      <th>sky</th>\n      <th>toast</th>\n      <th>today</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoO5479ElNUR",
        "colab_type": "text"
      },
      "source": [
        "Por ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "miJEM4PDlS7W",
        "outputId": "ca79efd6-db7f-449a-e05c-71a9727318c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "print(norm_corpus[6], '\\n')\n",
        "print(tf_df.loc[6])"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "sky blue sky beautiful today \n\nbacon        0\nbeans        0\nbeautiful    1\nblue         1\nbreakfast    0\nbrown        0\ndog          0\neggs         0\nfox          0\ngreen        0\nham          0\njumps        0\nkings        0\nlazy         0\nlove         0\nquick        0\nsausages     0\nsky          2\ntoast        0\ntoday        1\nName: 6, dtype: int64\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FdLpMNUcj-pU"
      },
      "source": [
        "Se puede ver claramente que cada columna o dimensión representa una palabra en el corpus y cada fila representa un documento. Cada celda representa el número de veces que una palabra (columna) aparece en un documento (fila).\n",
        "\n",
        "\n",
        "**IMPORTANTE:** Al utilizar la representación de Bolsa de Palabras, se recomienda normalizar los valores de las frecuencias de los términos. Para esto, se divide el número de apariciones de una palabra por el número total de palabras en el documento. De esta forma, la cantidad de veces que aparece una palabra en documentos grandes se puede comparar con la de documentos más pequeños.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BMD7Fmq4ksSW"
      },
      "source": [
        "### 2.2 TF-IDF\n",
        "\n",
        "Una de las principales desventajas de las bolsas de palabras es que asumen que todos los términos tienen igual importancia. No obstante, existen dos casos en los que algunos términos deberían tener mayor o menor importancia:\n",
        "\n",
        "1. Palabras comunes que aparecen en todos los documentos y no aportan mucha información para distinguir un documento de otro. \n",
        "2. Términos únicos y poco frecuentes que son sumamente relevantes para distinguir algunos documentos en específico.\n",
        "\n",
        "En este ámbito, surge la necesidad de estrategias que permitan capturar este tipo de relaciones. Una de las soluciones más comunes es **term frequency - inverse document frequency (TF-IDF)**. Se trata de un método que fue propuesto como métrica para la evaluación de resultados en motores de búsqueda y se convirtió en un estándar dentro de los sistemas de recuperación de información.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN7w8VUrwBkS",
        "colab_type": "text"
      },
      "source": [
        "**TF-IDF** amplía la idea de bolsas de palabras al ponderar cada término $t_i$ por un peso $w_{i}$ como se muestra a continuación:\n",
        "\n",
        "$$\n",
        "tfidf(t_i|d_j)=\\mathbf{Tf}(t_i,d_j)w_i\n",
        "$$\n",
        "\n",
        "Con esto, se puede asignar un peso menor a aquellos términos comunes entre documentos y un peso mayor a aquellos términos poco frecuentes. Una de las formas más comunes para determinar estos pesos es con la **frecuencia inversa de documento** $w_{idf}(t_i)$ que se calcula así:\n",
        "\n",
        "$$\n",
        "w_{idf}(t_i)=1+\\log{\\frac{N}{1+df(t_i)}}\n",
        "$$\n",
        "\n",
        "Donde $N$ es el número de documentos en el corpus y $df(t_i)$ es el número de documentos en los que se encuentra el término $t_i$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-M7sj3xYwUQb",
        "colab_type": "text"
      },
      "source": [
        "Adicionalmente, una extención de TF-IDF consiste en un cambio de escala de la matríz $\\mathbf{Tf}$, con esto, se busca atenuar el impacto que tienen los términos que aparecen muchas veces en un documento. Esto se consigue utilizando *sub-linear scaling* $wf(\\mathbf{Tf}(t_i,d_j))$ y consiste en transformar las ocurrencias a una escala logarítmica donde los valores grandes se ven atenuados:\n",
        "\n",
        "$$\n",
        "wf(\\mathbf{Tf}(t_i,d_j)) = \\left\\{\n",
        "\t       \\begin{array}{cl}\n",
        "\t\t 1+\\log{\\mathbf{Tf}(t_i,d_j)}      & \\mathrm{si\\ } \\mathbf{Tf}(t_i,d_j) > 0 \\\\\n",
        "\t\t 0 & \\mathrm{si\\ } \\mathbf{Tf}(t_i,d_j) \\le 0 \\\\\n",
        "\t       \\end{array}\n",
        "\t     \\right.\n",
        "$$\n",
        "\n",
        "Finalmente, una versión de TF-IDF con *sub-linear scaling* se muestra a continuación:\n",
        "\n",
        "$$\n",
        "tfidf_{2}(t_i|d_j)=wf(\\mathbf{Tf}(t_i,d_j))w_{idf}(t_i)\n",
        "$$\n",
        "\n",
        "Más info: [**tf-idf**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf).\n",
        "\n",
        "Veamos cómo obtener una representación TF-IDF usando scikit-learn:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ql5RsD-nuIws",
        "colab": {}
      },
      "source": [
        "# Importamos el extractor de características tf-idf de scikit-learn \n",
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QnE7widGuWqq",
        "colab": {}
      },
      "source": [
        "# Creamos el objeto tf-idf transformer\n",
        "tt = TfidfTransformer(use_idf=True, sublinear_tf=True)\n",
        "# Obtenemos la matríz tf-idf utilizando la bolsa de palabras que extragimos anteriormente \n",
        "tt_matrix = tt.fit_transform(cv_matrix)\n",
        "tt_matrix"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<8x20 sparse matrix of type '<class 'numpy.float64'>'\n\twith 42 stored elements in Compressed Sparse Row format>"
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CcTo9sLvvSwR",
        "outputId": "cf0fafd7-de05-4411-b3db-a9d9c537e87f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "# Mostramos la representación tf-idf\n",
        "tt_matrix = tt_matrix.toarray()\n",
        "vocab = cv.get_feature_names()\n",
        "pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n3   0.32   0.38       0.00  0.00       0.38   0.00  0.00  0.32  0.00   0.00   \n4   0.39   0.00       0.00  0.00       0.00   0.00  0.00  0.39  0.00   0.47   \n5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n6   0.00   0.00       0.39  0.34       0.00   0.00  0.00  0.00  0.00   0.00   \n7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n\n    ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n0  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.60   0.00   0.00  \n1  0.00   0.00   0.00  0.00  0.57   0.00      0.00  0.49   0.00   0.00  \n2  0.00   0.53   0.00  0.38  0.00   0.38      0.00  0.00   0.00   0.00  \n3  0.32   0.00   0.38  0.00  0.00   0.00      0.32  0.00   0.38   0.00  \n4  0.39   0.00   0.00  0.00  0.39   0.00      0.39  0.00   0.00   0.00  \n5  0.00   0.00   0.00  0.42  0.00   0.42      0.00  0.00   0.00   0.00  \n6  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.66   0.00   0.54  \n7  0.00   0.00   0.00  0.45  0.00   0.45      0.00  0.00   0.00   0.00  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bacon</th>\n      <th>beans</th>\n      <th>beautiful</th>\n      <th>blue</th>\n      <th>breakfast</th>\n      <th>brown</th>\n      <th>dog</th>\n      <th>eggs</th>\n      <th>fox</th>\n      <th>green</th>\n      <th>ham</th>\n      <th>jumps</th>\n      <th>kings</th>\n      <th>lazy</th>\n      <th>love</th>\n      <th>quick</th>\n      <th>sausages</th>\n      <th>sky</th>\n      <th>toast</th>\n      <th>today</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.60</td>\n      <td>0.53</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.60</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.49</td>\n      <td>0.43</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.57</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.49</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.53</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.32</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.32</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.32</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.32</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.39</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.39</td>\n      <td>0.00</td>\n      <td>0.47</td>\n      <td>0.39</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.39</td>\n      <td>0.00</td>\n      <td>0.39</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.37</td>\n      <td>0.00</td>\n      <td>0.42</td>\n      <td>0.42</td>\n      <td>0.00</td>\n      <td>0.42</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.42</td>\n      <td>0.00</td>\n      <td>0.42</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.39</td>\n      <td>0.34</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.66</td>\n      <td>0.00</td>\n      <td>0.54</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.45</td>\n      <td>0.45</td>\n      <td>0.00</td>\n      <td>0.45</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.45</td>\n      <td>0.00</td>\n      <td>0.45</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x9nWcLszvt_c"
      },
      "source": [
        "`TfidfTransformer` toma como entrada el vector de Bag of Words y lo transforma en su representación tf-idf. \n",
        "\n",
        "**IMPORTANTE**: No es necesario calcular el modelo BoW antes de obtener la representación tf-idf. Con la clase `TfidfVectorizer` podremos obtener la representación tf-idf directamente del texto como se muestra a continuación. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tDGpnklkxcLP",
        "colab": {}
      },
      "source": [
        "# Importamos el extractor de características de scikit-learn \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RA8XLTHfxo3B",
        "colab": {}
      },
      "source": [
        "# Instanciamos la clase \n",
        "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True, sublinear_tf=True)\n",
        "# Entrenamos y extraemos características \n",
        "tv_matrix= tv.fit_transform(norm_corpus)\n",
        "tv_matrix"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<8x20 sparse matrix of type '<class 'numpy.float64'>'\n\twith 42 stored elements in Compressed Sparse Row format>"
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HvDjmKZPyBtO",
        "outputId": "c8e62cd0-c25c-4408-d82f-ce9ea9c49e89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "# Visualizamos el resultado tf-idf obtenido\n",
        "tf_idf_matrix= tv_matrix.toarray()\n",
        "vocab = tv.get_feature_names()\n",
        "pd.DataFrame(np.round(tf_idf_matrix, 2), columns=vocab)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "   bacon  beans  beautiful  blue  breakfast  brown   dog  eggs   fox  green  \\\n0   0.00   0.00       0.60  0.53       0.00   0.00  0.00  0.00  0.00   0.00   \n1   0.00   0.00       0.49  0.43       0.00   0.00  0.00  0.00  0.00   0.00   \n2   0.00   0.00       0.00  0.00       0.00   0.38  0.38  0.00  0.38   0.00   \n3   0.32   0.38       0.00  0.00       0.38   0.00  0.00  0.32  0.00   0.00   \n4   0.39   0.00       0.00  0.00       0.00   0.00  0.00  0.39  0.00   0.47   \n5   0.00   0.00       0.00  0.37       0.00   0.42  0.42  0.00  0.42   0.00   \n6   0.00   0.00       0.39  0.34       0.00   0.00  0.00  0.00  0.00   0.00   \n7   0.00   0.00       0.00  0.00       0.00   0.45  0.45  0.00  0.45   0.00   \n\n    ham  jumps  kings  lazy  love  quick  sausages   sky  toast  today  \n0  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.60   0.00   0.00  \n1  0.00   0.00   0.00  0.00  0.57   0.00      0.00  0.49   0.00   0.00  \n2  0.00   0.53   0.00  0.38  0.00   0.38      0.00  0.00   0.00   0.00  \n3  0.32   0.00   0.38  0.00  0.00   0.00      0.32  0.00   0.38   0.00  \n4  0.39   0.00   0.00  0.00  0.39   0.00      0.39  0.00   0.00   0.00  \n5  0.00   0.00   0.00  0.42  0.00   0.42      0.00  0.00   0.00   0.00  \n6  0.00   0.00   0.00  0.00  0.00   0.00      0.00  0.66   0.00   0.54  \n7  0.00   0.00   0.00  0.45  0.00   0.45      0.00  0.00   0.00   0.00  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>bacon</th>\n      <th>beans</th>\n      <th>beautiful</th>\n      <th>blue</th>\n      <th>breakfast</th>\n      <th>brown</th>\n      <th>dog</th>\n      <th>eggs</th>\n      <th>fox</th>\n      <th>green</th>\n      <th>ham</th>\n      <th>jumps</th>\n      <th>kings</th>\n      <th>lazy</th>\n      <th>love</th>\n      <th>quick</th>\n      <th>sausages</th>\n      <th>sky</th>\n      <th>toast</th>\n      <th>today</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.60</td>\n      <td>0.53</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.60</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.49</td>\n      <td>0.43</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.57</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.49</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.53</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.32</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.32</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.32</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.32</td>\n      <td>0.00</td>\n      <td>0.38</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.39</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.39</td>\n      <td>0.00</td>\n      <td>0.47</td>\n      <td>0.39</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.39</td>\n      <td>0.00</td>\n      <td>0.39</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.37</td>\n      <td>0.00</td>\n      <td>0.42</td>\n      <td>0.42</td>\n      <td>0.00</td>\n      <td>0.42</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.42</td>\n      <td>0.00</td>\n      <td>0.42</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.39</td>\n      <td>0.34</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.66</td>\n      <td>0.00</td>\n      <td>0.54</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.45</td>\n      <td>0.45</td>\n      <td>0.00</td>\n      <td>0.45</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.45</td>\n      <td>0.00</td>\n      <td>0.45</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xwP2rBv1GXG",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2.1 Medidas de similitud: Norma euclidiana"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tangG9QeMCcx",
        "colab_type": "text"
      },
      "source": [
        "Como se mencionó anteriormente, **TF-IDF** es un método que surgió en sistemas de recuperación de información, ya que, **permite representar un documento en un espacio vectorial continúo** $\\mathbb{R}^{|v|}$ donde los documentos se pueden comparar de forma numérica utilizando alguna **medida de similitud**. \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/larajuse/share/master/vector.png\" alt=\"vector\" width=\"500\"/>\n",
        "\n",
        "Una de las formas más simples para comparar la representación de dos documentos $\\vec{x}$ y $\\vec{y}$ es por medio de la **distancia entre los vectores o la [norma euclidiana](https://en.wikipedia.org/wiki/Norm_%28mathematics%29#Euclidean_norm)** $||\\vec{x}-\\vec{y}||$. No obstante, esta métrica no funciona tan bien con TF-IDF, esto se debe principalmente a que en esta representación importan más los términos comunes que los términos continúos (que en una representación de bolsa de palabras, no tienen ningún orden) y las magnitudes. Veamos un ejemplo de lo anterior:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmpUOcDbS23e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# definimos tres oraciones:\n",
        "X = [\"new in america\",\n",
        "     \"i am new here but america is a great place to live\",\n",
        "     \"spectrometry is an indispensable new tool\"]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "{'a',\n 'am',\n 'america',\n 'an',\n 'but',\n 'great',\n 'here',\n 'i',\n 'in',\n 'indispensable',\n 'is',\n 'live',\n 'new',\n 'place',\n 'spectrometry',\n 'to',\n 'tool'}"
          },
          "metadata": {},
          "execution_count": 121
        }
      ],
      "source": [
        "set(' '.join(X).split(' '))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3u9YIqKWTpmZ",
        "colab_type": "code",
        "outputId": "1a4195bb-b7ff-44b3-f82b-4b2e0bd23960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# Representamos los ejemplos\n",
        "vect = TfidfVectorizer(norm = None)\n",
        "X_tfidf = vect.fit_transform(X).toarray()\n",
        "pd.DataFrame(np.round(X_tfidf, 2), columns=vect.get_feature_names())"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "     am  america    an   but  great  here    in  indispensable    is  live  \\\n0  0.00     1.29  0.00  0.00   0.00  0.00  1.69           0.00  0.00  0.00   \n1  1.69     1.29  0.00  1.69   1.69  1.69  0.00           0.00  1.29  1.69   \n2  0.00     0.00  1.69  0.00   0.00  0.00  0.00           1.69  1.29  0.00   \n\n   new  place  spectrometry    to  tool  \n0  1.0   0.00          0.00  0.00  0.00  \n1  1.0   1.69          0.00  1.69  0.00  \n2  1.0   0.00          1.69  0.00  1.69  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>am</th>\n      <th>america</th>\n      <th>an</th>\n      <th>but</th>\n      <th>great</th>\n      <th>here</th>\n      <th>in</th>\n      <th>indispensable</th>\n      <th>is</th>\n      <th>live</th>\n      <th>new</th>\n      <th>place</th>\n      <th>spectrometry</th>\n      <th>to</th>\n      <th>tool</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00</td>\n      <td>1.29</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>1.69</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.69</td>\n      <td>1.29</td>\n      <td>0.00</td>\n      <td>1.69</td>\n      <td>1.69</td>\n      <td>1.69</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>1.29</td>\n      <td>1.69</td>\n      <td>1.0</td>\n      <td>1.69</td>\n      <td>0.00</td>\n      <td>1.69</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>1.69</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>1.69</td>\n      <td>1.29</td>\n      <td>0.00</td>\n      <td>1.0</td>\n      <td>0.00</td>\n      <td>1.69</td>\n      <td>0.00</td>\n      <td>1.69</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2888zI5T-YD",
        "colab_type": "text"
      },
      "source": [
        "Ahora, miremos cual documento está más cerca al documento 0 (más cerca significa menor distancia euclidiana)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptDMJi4LUYc7",
        "colab_type": "code",
        "outputId": "0543a887-64c3-464c-8ca4-15ba4aaefd73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "d1 = np.linalg.norm(X_tfidf[0] - X_tfidf[1])\n",
        "d2 = np.linalg.norm(X_tfidf[0] - X_tfidf[2])\n",
        "print(\"La distancia euclidiana entre el documento 0 y el documento 1 es: {}\".format(d1))\n",
        "print(\"La distancia euclidiana entre el documento 0 y el documento 2 es: {}\".format(d2))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "La distancia euclidiana entre el documento 0 y el documento 1 es: 4.959042661645336\nLa distancia euclidiana entre el documento 0 y el documento 2 es: 4.201188773980275\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU_bpieYc-0Y",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que el documento 2 está más cerca al documento 0, sin embargo, fácilmente podemos ver que el documento 1 y el documento 0 son semánticamente más parecidos. Esto se debe a que la distancia euclidiana no tiene en cuenta la distribución de los términos, sólo tiene en cuenta los puntos más cercanos en este espacio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM1cJ2NM1hId",
        "colab_type": "text"
      },
      "source": [
        "#### 2.2.2 Medidas de similitud: coseno"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY7-6GRz1eea",
        "colab_type": "text"
      },
      "source": [
        "Una alternativa es la **[similitud coseno](https://es.wikipedia.org/wiki/Similitud_coseno)**, la cual es más apropiada para representaciones basadas en histogramas como TF-IDF, ya que, es una medida del alineamiento de dos vectores.\n",
        "\n",
        "$$\n",
        "cosine(\\vec{x},\\vec{y})=\\frac{\\vec{x}\\cdot\\vec{y}}{||\\vec{x}||~||\\vec{y}||}\n",
        "$$\n",
        "\n",
        "Veamos el mismo ejemplo, pero con la similitud coseno:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AkPW8JG3LLp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnghfpL5U-uR",
        "colab_type": "code",
        "outputId": "33f2c597-4dca-49f0-e6b1-c47971b90be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "d1 = cosine_similarity(X_tfidf[0].reshape(1,-1), X_tfidf[1].reshape(1,-1)).flatten()\n",
        "d2 = cosine_similarity(X_tfidf[0].reshape(1,-1),X_tfidf[2].reshape(1,-1)).flatten()\n",
        "print(f\"La similitud coseno entre el documento 0 y el documento 1 es {d1[0]}\")\n",
        "print(f\"La similitud coseno entre el documento 0 y el documento 2 es {d2[0]}\")"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "La similitud coseno entre el documento 0 y el documento 1 es 0.22901631859761276\nLa similitud coseno entre el documento 0 y el documento 2 es 0.11319907547001144\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q8O8_Jm3b1E",
        "colab_type": "code",
        "outputId": "bc3f74a9-2ddd-4d72-cbc0-a14738779984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "X_tfidf[0]"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([0.        , 1.28768207, 0.        , 0.        , 0.        ,\n       0.        , 1.69314718, 0.        , 0.        , 0.        ,\n       1.        , 0.        , 0.        , 0.        , 0.        ])"
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkiKu62N4hjr",
        "colab_type": "code",
        "outputId": "84fce69f-d842-4fa0-c138-754487f21d1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "X_tfidf[0].reshape(1,-1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 1.28768207, 0.        , 0.        , 0.        ,\n",
              "        0.        , 1.69314718, 0.        , 0.        , 0.        ,\n",
              "        1.        , 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjA9l6jg3Vvy",
        "colab_type": "code",
        "outputId": "49c2865f-0374-4700-f98f-757789c652ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "X_tfidf[0].reshape(1,15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 1.28768207, 0.        , 0.        , 0.        ,\n",
              "        0.        , 1.69314718, 0.        , 0.        , 0.        ,\n",
              "        1.        , 0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFK3fzaQe_4S",
        "colab_type": "text"
      },
      "source": [
        "En este caso, el documento 1 está más cerca que el documento 2 (más cerca en similitud coseno significa valores cercanos a 1, lo que equivale a un ángulo cercano a cero). En general, la similitud coseno funciona mejor en este tipo de representaciones, por ello, es la más ampliamente usada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCuxOrVz2xXA",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 N-grams\n",
        "\n",
        "Una alternativa a los métodos de conteo de palabras son los n-gramas, los cuales, consisten en conteos de secuencias de letras o de determinadas palabras. Es decir, se busca una distribución de secuencias $s_i$ dado un determinado documento $d_j$, así: $P(S=s_i|D=d_j)$\n",
        "\n",
        "* Un ejemplo de secuencias a nivel de caracter puede ser $S=\\{``uni\",``niv\",``ive\", ``ver\", ``ers\", ``rsi\", ``sid\", ``ida\", ``dad\"\\}$, que corresponden a los 3-gramas que conforman la palabra \"universidad\".\n",
        "\n",
        "* Un ejemplo a nivel de secuencias a nivel de palabras puede ser: $S=\\{``albert~einstein\", ``einstein~era\", ``era~un\", ``un~cientifico\"\\}$, lo que corresponde a los 2-gramas que conforman la frase \"albert einstein era un cientifico\".\n",
        "\n",
        "Veamos la implementación de este método:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjvuOSie5mmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3-grams a nivel caracter\n",
        "cv = CountVectorizer(ngram_range=(3,3), analyzer=\"char\")\n",
        "# Extraemos las características del corpus \n",
        "cv_matrix = cv.fit_transform([\"universidad\", \"universo\"])"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prE_OAEK7Fcl",
        "colab_type": "code",
        "outputId": "94b098a8-9db2-4478-b611-f1a4fda8dd76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "pd.DataFrame(data=cv_matrix.toarray(), columns=cv.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dad</th>\n",
              "      <th>ers</th>\n",
              "      <th>ida</th>\n",
              "      <th>ive</th>\n",
              "      <th>niv</th>\n",
              "      <th>rsi</th>\n",
              "      <th>rso</th>\n",
              "      <th>sid</th>\n",
              "      <th>uni</th>\n",
              "      <th>ver</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   dad  ers  ida  ive  niv  rsi  rso  sid  uni  ver\n",
              "0    1    1    1    1    1    1    0    1    1    1\n",
              "1    0    1    0    1    1    0    1    0    1    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_5wyjDi64V4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3-grams a nivel palabra\n",
        "cv = CountVectorizer(ngram_range=(2,2), analyzer=\"word\")\n",
        "# Extraemos las características del corpus \n",
        "cv_matrix = cv.fit_transform([\"albert einstein era un cientifico\", \"la teoría de la relatividad de albert einstein\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiVA2Z3O69p-",
        "colab_type": "code",
        "outputId": "6135ff50-5e11-46b9-be1a-fac683ff4ada",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "pd.DataFrame(data=cv_matrix.toarray(), columns=cv.get_feature_names())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>albert einstein</th>\n",
              "      <th>de albert</th>\n",
              "      <th>de la</th>\n",
              "      <th>einstein era</th>\n",
              "      <th>era un</th>\n",
              "      <th>la relatividad</th>\n",
              "      <th>la teoría</th>\n",
              "      <th>relatividad de</th>\n",
              "      <th>teoría de</th>\n",
              "      <th>un cientifico</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   albert einstein  de albert  de la  ...  relatividad de  teoría de  un cientifico\n",
              "0                1          0      0  ...               0          0              1\n",
              "1                1          1      1  ...               1          1              0\n",
              "\n",
              "[2 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFJ5ZkPrk_Xk",
        "colab_type": "text"
      },
      "source": [
        "### Ejercicio\n",
        "\n",
        "* Bajar el conjunto de documentos de kaggle https://www.kaggle.com/uciml/sms-spam-collection-dataset  o de\n",
        "https://drive.google.com/file/d/1HUXG6b7U3_taJw0-Aj6bkl39gdk6h5AK/view?usp=sharing\n",
        "\n",
        "* Cargarlo el data set\n",
        "\n",
        "* Construir la matriz tf, tf_idf, y trigramas por palabras\n",
        "\n",
        "* Seleccionar cinco tweets y clasificarlos con los 5 vecinos más cercanos usando tf_idf con similitud coseno\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9njwQdJzRVG",
        "colab_type": "code",
        "outputId": "5c2df912-c6ba-4536-a3a9-2b9bd3c2b78d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "# Cargamos el dataset como un dataframe de pandas\n",
        "dataset_df = pd.read_csv(\"../Files/spam.csv\", usecols=[0, 1])\n",
        "dataset_df.head(10)"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "     v1  \\\n0   ham   \n1   ham   \n2  spam   \n3   ham   \n4   ham   \n5  spam   \n6   ham   \n7   ham   \n8  spam   \n9  spam   \n\n                                                                                                                                                                 v2  \n0                                                   Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  \n1                                                                                                                                     Ok lar... Joking wif u oni...  \n2       Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  \n3                                                                                                                 U dun say so early hor... U c already then say...  \n4                                                                                                     Nah I don't think he goes to usf, he lives around here though  \n5               FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, �1.50 to rcv  \n6                                                                                     Even my brother is not like to speak with me. They treat me like aids patent.  \n7  As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune  \n8     WINNER!! As a valued network customer you have been selected to receivea �900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.  \n9        Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&amp;C's apply 08452810075over18's</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives around here though</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>spam</td>\n      <td>FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, �1.50 to rcv</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnaminunginte Nurungu Vettam)' has been set as your callertune for all Callers. Press *9 to copy your friends Callertune</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>spam</td>\n      <td>WINNER!! As a valued network customer you have been selected to receivea �900 prize reward! To claim call 09061701461. Claim code KL341. Valid 12 hours only.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>spam</td>\n      <td>Had your mobile 11 months or more? U R entitled to Update to the latest colour mobiles with camera for Free! Call The Mobile Update Co FREE on 08002986030</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No_Sdu454ao1",
        "colab_type": "code",
        "outputId": "17f2d104-0c70-4d0a-cacb-aa0c2895342a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "dataset_df.count()"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "v1    5572\nv2    5572\ndtype: int64"
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBSa3Stx4gLJ",
        "colab_type": "code",
        "outputId": "e79eb55b-8d5c-4a86-d564-0b4c9dc7bdbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "dataset_df['v1'].value_counts()"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "ham     4825\nspam     747\nName: v1, dtype: int64"
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWLB2KT24hkr",
        "colab_type": "code",
        "outputId": "3e58aaa0-ad81-427f-cd3c-be13687fbb3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"% clase 'ham' :\", round(dataset_df['v1'].value_counts()[0]/dataset_df.count()[0]*100, 1))\n",
        "print(\"% clase 'spam':\", round(dataset_df['v1'].value_counts()[1]/dataset_df.count()[0]*100, 1))"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "% clase 'ham' : 86.6\n% clase 'spam': 13.4\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_h2sa0Y416L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XFUdjhR4ypYh"
      },
      "source": [
        "### 2.4 Word2Vec \n",
        "\n",
        "Uno de los problemas de los modelos basados en bolsas de palabras es que no tienen en cuenta características importantes del texto como la estructura, el contexto y el significado. Una alternativa es utilizar métodos de embedding como Word2vec, se trata de un método creado por Google en 2013 que está basado en deep learning y busca transformar las palabras en vectores numéricos dentro de un espacio vectorial que captura información **contextual** y **semántica**. \n",
        "\n",
        "● Objetivo: Construir un vector que represente las palabras de un corpus usando “distancias”\n",
        "\n",
        "● Usa una arquitectura de Red Neuronal para computar eficientemente vectores que representan palabras en grandes conjuntos de datos\n",
        "\n",
        "● Usa los pesos que aprende la red neuronal o minimización de funciones de\n",
        "costo, como “Word embedding”\n",
        "\n",
        "La representación de “Word embedding” puede revelar muchas relaciones\n",
        "ocultas entre palabras. **Por ejemplo**,\n",
        "vector (\"gato\") - vector (\"gatito\") es similar a vector (\"perro\") - vector (\"cachorro\")."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y0QzoliHKyz",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://mlwhiz.com/images/word2vec.png\" alt=\"word2vec\" width=\"800\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_ikjyDPHPHH",
        "colab_type": "text"
      },
      "source": [
        "Existen dos enfoques para construír un modelo Word2Vec:\n",
        "\n",
        "* **Skip-gram**: en este modelo, la entrada es la palabra que se quiere representar y las salidas son las palabras que la rodean. Por ejemplo, en la frase \"aprendiendo procesamiento de lenguaje natural\", si la entrada es la palabra \"de\", entonces las salidas serán \"aprendiendo\", \"procesamiento\", \"lenguaje\", \"natural\" si el tamaño del vecindario es 5. La arquitectura de la red neuronal para skip-gram se muestra a continuación:\n",
        "\n",
        "![skipgram](https://miro.medium.com/max/988/0*xqhh7Gd64VAQ1Sny.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJAzwJuCIDNK",
        "colab_type": "text"
      },
      "source": [
        "* **CBOW**: el modelo *continuous bag-of-words* (CBOW) es muy parecido al modelo skip-gram, la diferencia es que la entrada y la salida se intercambian, es decir, se utiliza el contexto (palabras en el vecindario) para predecir una palabra en específico. La arquitectura se muestra a continuación:\n",
        "\n",
        "![cbow](https://miro.medium.com/max/413/1*UdLFo8hgsX0a1NKKuf_n9Q.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV9Hpl-MHRE9",
        "colab_type": "text"
      },
      "source": [
        "Veamos la implementación de word2vec:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bsxk6noN2LoR",
        "colab": {}
      },
      "source": [
        "# Para entrenar el modelo word2vec necesitamos un corpus grande\n",
        "from nltk.corpus import gutenberg\n",
        "# Importamos módulo para manejar la puntuación\n",
        "from string import punctuation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZR6V_iLv5rYv",
        "colab": {}
      },
      "source": [
        "# En este caso usaremos la biblia como corpus\n",
        "bible = gutenberg.sents(\"bible-kjv.txt\")\n",
        "# Definimos los términos que no vamos a incluír:\n",
        "remove_terms = punctuation + \"0123456789\"\n",
        "print(\"caracteres inválidos: {}\".format(remove_terms))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b5PK2GFm6P_5",
        "colab": {}
      },
      "source": [
        "# Normalizamos el texto\n",
        "norm_bible = [ [word.lower() for word in sent if word not in remove_terms] for sent in bible]\n",
        "norm_bible = [' '.join(tok_sent) for tok_sent in norm_bible]\n",
        "norm_bible = filter(None, normalize_corpus(norm_bible))\n",
        "norm_bible = [tok_sent for tok_sent in norm_bible if len(tok_sent.split()) > 2]\n",
        "# imprimimos el número total de filas, un ejemplo de linea raw y procesada\n",
        "print('Total lines:', len(bible))\n",
        "print('\\nSample line:', bible[10])\n",
        "print('\\nProcessed line:', norm_bible[10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BqoRPXJl7-yz",
        "colab": {}
      },
      "source": [
        "# tokenizamos oraciones del corpus\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "tokenized_corpus = [wpt.tokenize(document) for document in norm_bible]\n",
        "tokenized_corpus[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWysAZliKGyO",
        "colab_type": "text"
      },
      "source": [
        "#### 2.4.1 Gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eXN9D7TK6wmV"
      },
      "source": [
        "En este caso utilizaremos **[gensim](https://radimrehurek.com/gensim/models/word2vec.html)** para entrenar el modelo **word2vec**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E5Nr0PpM7pIi",
        "colab": {}
      },
      "source": [
        "# Instalamos el módulo word2vec\n",
        "!pip install gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F8KRas1t7Wv7",
        "colab": {}
      },
      "source": [
        "# Importamos word2vec from gemsim\n",
        "from gensim.models import word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C8Rs3jVA8Lgx"
      },
      "source": [
        "Los siguientes parámetros son utilizados por el modelo Word2vec para construir el modelo:\n",
        "\n",
        "* feature_size: Determina la dimensión de los vectores de embedding.\n",
        "* window_context: Es el número de palabras en el vecindario que constituye el contexto.\n",
        "* min_word_count: Especifica el conteo mínimo de una palabra dentro del corpus para ser incluida dentro del vocabulario.\n",
        "* sample: este parámetro es usado para el sub-muestreo dentro del algoritmo. Generalmente, los valores entre 0.01 entre 0.0001 funcionan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pPawqV9c8GSD",
        "colab": {}
      },
      "source": [
        "feature_size = 100 \n",
        "window_context = 30 \n",
        "min_word_count = 1\n",
        "sample = 1e-3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nWOUdjPo9fkz",
        "colab": {}
      },
      "source": [
        "# definimos el modelo y lo entrenamos\n",
        "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size,\n",
        "                              window=window_context, min_count=min_word_count,\n",
        "                              sample=sample, iter=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GIG8AluM9pJC",
        "colab": {}
      },
      "source": [
        "# Con gensim podemos encontrar las palabras más similares a determinado término, por ejemplo:\n",
        "w2v_model.wv.most_similar(\"egypt\", topn=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "memK_SZEIZ1u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Podemos encontrar relaciones semánticas entre palabras:\n",
        "w2v_model.wv.most_similar(positive=['king', 'queen'], negative=['man'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TUcZI1Wl-GIb"
      },
      "source": [
        "Para verificar que las palabras similares se representan como vectores similares utilizaremos una técnica de reducción de dimensionalidad para visualizar las representaciones en un espacio bidimensional. En este caso utilizaremos t-SNE, el cual es uno de los algoritmos más utilizados para esta tarea y tiene las siguientes propiedades:\n",
        "\n",
        "* Se trata de una transformación no-lineal que conserva relaciones líneales entre puntos en un espacio de alta dimensionalidad.\n",
        "* t-SNE crea una distribución de probabilidad utilizando la distribución normal para definir relaciones entre los puntos en el espacio de alta dimensionalidad.\n",
        "* Utiliza la distribución t-student en el espacio de baja dimensionalidad con el fin de obtener una representación más dispersa (previene la [maldición de la dimensionalidad o el efecto Hughes](https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n)).\n",
        "* Es un método basado en descenso de gradiente que se utiliza para optimizar una función no convexa, por ello, puede converger a un mínimo local y puede requerir varias repeticiones para obtener un buen resultado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bKbDL62q-pgw",
        "colab": {}
      },
      "source": [
        "# Para reducir la dimensionalidad del word embedding\n",
        "from sklearn.manifold import TSNE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gWy82APU-xih",
        "colab": {}
      },
      "source": [
        "# Obtenemos las palabras similares \n",
        "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
        "                 for search_term in ['god', 'life', 'death','egypt', 'sin']}\n",
        "\n",
        "words = sum([[k] + v for k, v in similar_words.items()], [])\n",
        "wvs = w2v_model.wv[words]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7sU-Mv7KYr3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8ATK40Ax-5TG",
        "colab": {}
      },
      "source": [
        "# Reducimos la dimensionalidad\n",
        "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=2)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(wvs)\n",
        "labels = words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8fcatJ00-_F4",
        "colab": {}
      },
      "source": [
        "# Visualizamos \n",
        "plt.figure(figsize=(14, 8))\n",
        "plt.scatter(T[:, 0], T[:, 1], c='orange', edgecolors='r')\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwvyE-fYTfTr",
        "colab_type": "text"
      },
      "source": [
        "#### 2.4.2 Spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcIhcUp_UsUB",
        "colab_type": "text"
      },
      "source": [
        "Para utilizar las representaciones vectoriales de las palabras, debemos cargar un modelo de lenguaje más grande que el que hemos venido usando con SpaCy. En el siguiente ejemplo, trabajaremos con el https://spacy.io/models/es#es_core_news_md en **Español**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoyh5Yhdgd3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m spacy download es_core_news_md"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-bZOz35gQ-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importamos spaCy y cargamos los modelos\n",
        "import spacy\n",
        "import es_core_news_md\n",
        "nlp = es_core_news_md.load()  # Para español - la versión md (74MB) incluye word embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Vml1rPOVgQ-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nlp(u'perro').vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OhLzhXXgQ-P",
        "colab_type": "text"
      },
      "source": [
        "Los objetos Doc y Span tienen vectores, derivados de los promedios de los vectores de tokens individuales. Esto permite comparar similitudes entre documentos completos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvTw_mmkgQ-R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc = nlp(u'El perro blanco estaba ladrando en el parque.')\n",
        "\n",
        "doc.vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IFbj6qDgQ-U",
        "colab_type": "text"
      },
      "source": [
        "**Simulitud de vectores:**\n",
        "\n",
        "Más info: https://spacy.io/usage/vectors-similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Bpwu6g7bgQ-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = nlp(u'tigre gato perro mascota felino')\n",
        "\n",
        "# Combinaciones\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW7egkGXgQ-i",
        "colab_type": "text"
      },
      "source": [
        "Los **antónimos** no son necesariamente vectores muy diferentes. A veces aparecen en los mismos contextos por lo que podrían tener vectores similares."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhiFw4qDgQ-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = nlp(u'amar odiar')\n",
        "\n",
        "# Combinaciones\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k15jz_MeaDbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = nlp(u'frío calor')\n",
        "\n",
        "# Combinaciones\n",
        "for token1 in tokens:\n",
        "    for token2 in tokens:\n",
        "        print(token1.text, token2.text, token1.similarity(token2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FY7eaNtgQ-r",
        "colab_type": "text"
      },
      "source": [
        "**Aritmética de vectores**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cECVsC3kgQ-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import spatial\n",
        "\n",
        "cosine_similarity = lambda x, y: 1 - spatial.distance.cosine(x, y)\n",
        "\n",
        "profe = nlp.vocab['profesor'].vector\n",
        "hombre = nlp.vocab['hombre'].vector\n",
        "mujer = nlp.vocab['mujer'].vector\n",
        "\n",
        "# Podemos encontrar el vector más cercano en el vocabulario para el resultado \"profe\" - \"hombre\" + \"mujer\"\n",
        "nuevo_vector = profe - hombre + mujer\n",
        "computed_similarities = []\n",
        "\n",
        "for word in nlp.vocab:\n",
        "    # Ignore words without vectors and mixed-case words:\n",
        "    if word.has_vector:\n",
        "        if word.is_lower:\n",
        "            if word.is_alpha:\n",
        "                similarity = cosine_similarity(nuevo_vector, word.vector)\n",
        "                computed_similarities.append((word, similarity))\n",
        "\n",
        "computed_similarities = sorted(computed_similarities, key=lambda item: -item[1])\n",
        "\n",
        "print([w[0].text for w in computed_similarities[:10]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w9wU0losJ2U",
        "colab_type": "text"
      },
      "source": [
        "### 2.5 FastText\n",
        "\n",
        "Se trata de una extensión de Word2Vec que fue propuesta por facebook en el 2016. En este caso, en lugar de utilizar palabras individuales, los textos se dividen en n-gramas de caracteres. Por ejemplo, los 3-gramas para la palabra \"universidad\" son: \"uni\", \"niv\", \"ive\", \"ver\", \"ers\", \"rsi\", \"sid\", \"ida\", \"dad\". La representación de una palabra es la suma de estos n-gramas y es utilizada para entrenar una red neuronal. Este enfoque permite resaltar palabras raras, fundamentalmente, porque es muy probable que sus n-gramas también aparezcan en otras palabras.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiR1ZHRAsJ2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importamos el modelo FastText\n",
        "from gensim.models import FastText"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmuZsHQGsJ2t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ft_model = FastText(tokenized_corpus, size=feature_size, # Volvemos a usar el corpus de la Biblia\n",
        "                  window=window_context, min_count=min_word_count,\n",
        "                  iter=50, workers=4,sg=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8m99PLQsJ27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "similar_words = {search_term: [item[0] for item in w2v_model.wv.most_similar([search_term], topn=5)]\n",
        "                 for search_term in ['god', 'jesus', 'noah','egypt', 'john', 'gospel', 'moses','famine']}\n",
        "similar_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3GDuIKNsJ3K",
        "colab_type": "text"
      },
      "source": [
        "## 3. Ejercicio\n",
        "\n",
        "* Siga los pasos utilizados en word2vec (con GenSim) para visualizar y encontrar la representación vectorial de los textos con FastText"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBOBXCe2sJ3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL2NJ2_ZCkIO",
        "colab_type": "text"
      },
      "source": [
        "## Referencias\n",
        "\n",
        "* [1] Rahul Agarwal. NLP Learning Series: Part 1 - Text Preprocessing Methods for Deep\n",
        "Learning. 2019. url: https://mlwhiz.com/blog/2019/01/17/deeplearning%7B%\n",
        "5C_%7Dnlp%7B%5C_%7Dpreprocess/.\n",
        "* [2] M.J. Ashwin. Next Word Prediction Using Markov Model. 2018. url: https://\n",
        "medium.com/ymedialabs-innovation/next-word-prediction-using-markovmodel-570fc0475f96.\n",
        "* [3] FacebookOpenSource. FastText: Library for efficient text classification and representation learning. 2019. url: https://fasttext.cc/.\n",
        "* [4] Shreya Ghelani. From Word Embeddings to Pretrained Language Models A New\n",
        "Age In NLP - Part-1. 2016. url: https://towardsdatascience.com/from-wordembeddings- to- pretrained- language- models- a- new- age- in- nlp- part- 1-\n",
        "7ed0c7f3dfc5.\n",
        "* [5] Steeve Huang. Word2Vec and FastText Word Embedding with Gensim. 2018. url:\n",
        "https : / / towardsdatascience . com / word - embedding - with - word2vec - and -\n",
        "fasttext-a209c1d3e12c.\n",
        "* [6] Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. Introduction\n",
        "to Information Retrieval. New York, NY, USA: Cambridge University Press, 2008.\n",
        "isbn: 0521865719, 9780521865715.\n",
        "* [7] Gonzalo RuizdeVilla. Introducción a Word2vec (skip gram model). 2018. url: https:\n",
        "//medium.com/@gruizdevilla/introducci%7B%5C’%7Bo%7D%7Dn-a-word2vecskip-gram-model-4800f72c871f."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "representacionTexto (1).ipynb",
      "provenance": [],
      "collapsed_sections": [
        "A3GDuIKNsJ3K"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3.8.1 64-bit ('nlp': conda)",
      "language": "python",
      "name": "python38164bitnlpconda91879d213a784cd7ad50df7b2e78c33c"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6-final"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}