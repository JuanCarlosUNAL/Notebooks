{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiN42UQs_sU8",
        "colab_type": "text"
      },
      "source": [
        "# Análisis No Supervisado de Texto\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk9gKzITAjLq",
        "colab_type": "text"
      },
      "source": [
        "# Análisis no supervisado de texto\n",
        "\n",
        "Gran cantidad de los datos textuales que se generan no contienen etiquetas por lo que un modelo de clasisficación no es apropiado. Cuando no existen etiquetas en los textos lo que se desea es obtener conocimientos o patrones descriptivos del conjunto de documentos, es por eso que se  aplican modelos **descriptivos**, lo que se conoce como aprendizaje no supervisado. \n",
        "\n",
        "Una de las más importantes aplicaciones es la identificación de temas o tópicos que son comunes a varios documentos a traves de algoritmos de **clustering** usando diversidad de representaciones de los documentos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1rTPGvGmOq4",
        "colab_type": "text"
      },
      "source": [
        "**Ejemplos:**\n",
        "\n",
        "* *Música*: Identificar canciones similares por la letra de la canción, en lugar de usar el género, el artista, el año o el albúm.\n",
        "  \n",
        "* *Opinión*: Identificar opiniones o tendencias de marcas o noticias de los documentos generados en redes sociales o periódicos.\n",
        "  \n",
        "* Encontrar los géneros en un corpus conformado por todos los libros en una biblioteca.\n",
        "\n",
        "* Encontrar los *estilos* de escritores en posts de redes sociales.\n",
        "\n",
        "* Encontar géneros de películas en documentos con sinápsis.\n",
        "\n",
        "* *Bioinformática*:\n",
        "\n",
        "<img src=\"https://kldavenport.com/topic-modeling-amazon-reviews/image0.png\" alt=\"topic\" width=\"650\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y_kbAZ1mRki",
        "colab_type": "text"
      },
      "source": [
        "Como se mencionó anteriormente, una de las aproximaciones para encontrar tematicas son las técnicas de **clustering**. POr lo tanto, introduciremos algunos de los métodos clásicos de **clustering** aplicados a la identificación de tópicos: **K-means, affinity propagation y hierarchical clustering**. \n",
        "\n",
        "En esta sesión, utilizaremos el dataset [TMDB](https://www.kaggle.com/tmdb/tmdb-movie-metadata/download), el cual contiene resumenes de **películas** con anotaciones de géneros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAFMQIkb6l7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Se importan las librerías\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json, nltk, re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib inline\n",
        "plt.style.use(\"ggplot\")\n",
        "nltk.download(\"popular\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[nltk_data] Downloading collection 'popular'\n[nltk_data]    | \n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package cmudict is already up-to-date!\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package gazetteers is already up-to-date!\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package genesis is already up-to-date!\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package gutenberg is already up-to-date!\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package inaugural is already up-to-date!\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package movie_reviews is already up-to-date!\n[nltk_data]    | Downloading package names to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package names is already up-to-date!\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package shakespeare is already up-to-date!\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package treebank is already up-to-date!\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package twitter_samples is already up-to-date!\n[nltk_data]    | Downloading package omw to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package omw is already up-to-date!\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package wordnet is already up-to-date!\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package wordnet_ic is already up-to-date!\n[nltk_data]    | Downloading package words to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package words is already up-to-date!\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n[nltk_data]    | Downloading package punkt to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package snowball_data is already up-to-date!\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n[nltk_data]    |       to-date!\n[nltk_data]    | \n[nltk_data]  Done downloading collection popular\n"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "True"
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20RH5SBn7cq7",
        "colab_type": "text"
      },
      "source": [
        "Comenzaremos cargando el dataset y mostrando una descripción general:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHoDoobIoHJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"../files/movies.csv\")\n",
        "df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 4803 entries, 0 to 4802\nData columns (total 20 columns):\n #   Column                Non-Null Count  Dtype  \n---  ------                --------------  -----  \n 0   budget                4803 non-null   int64  \n 1   genres                4803 non-null   object \n 2   homepage              1712 non-null   object \n 3   id                    4803 non-null   int64  \n 4   keywords              4803 non-null   object \n 5   original_language     4803 non-null   object \n 6   original_title        4803 non-null   object \n 7   overview              4800 non-null   object \n 8   popularity            4803 non-null   float64\n 9   production_companies  4803 non-null   object \n 10  production_countries  4803 non-null   object \n 11  release_date          4802 non-null   object \n 12  revenue               4803 non-null   int64  \n 13  runtime               4801 non-null   float64\n 14  spoken_languages      4803 non-null   object \n 15  status                4803 non-null   object \n 16  tagline               3959 non-null   object \n 17  title                 4803 non-null   object \n 18  vote_average          4803 non-null   float64\n 19  vote_count            4803 non-null   int64  \ndtypes: float64(3), int64(4), object(13)\nmemory usage: 750.6+ KB\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-gIynwn7igw",
        "colab_type": "text"
      },
      "source": [
        "Nos enfocaremos en tres campos dentro del dataset: \"overview\", \"genres\" y \"title\". Adicionalmente, se realizará una tarea de limpieza de datos, como existen valores inválidos (NAN), utilizamos la función `dropna` para eliminarlos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtGYL-sjoLJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# filtramos valores faltantes en las columnas de interes\n",
        "df.dropna(axis=0, subset=[\"overview\", \"genres\", \"title\"], inplace=True)\n",
        "df['overview'].count()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "4800"
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLKNha5a8IuL",
        "colab_type": "text"
      },
      "source": [
        "Veamos como queda el dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX0GxCLOpwq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(5):\n",
        "    print(f\"{' ':8}Ejemplo {i+1}\")\n",
        "    print(f\"* Title: {df['title'][i]}\\n\")\n",
        "    print(f\"* Overview: {df['overview'][i]}\\n\")\n",
        "    print(f\"* Genres: {df['genres'][i]}\\n\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Ejemplo 1\n* Title: Avatar\n\n* Overview: In the 22nd century, a paraplegic Marine is dispatched to the moon Pandora on a unique mission, but becomes torn between following orders and protecting an alien civilization.\n\n* Genres: [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"name\": \"Fantasy\"}, {\"id\": 878, \"name\": \"Science Fiction\"}]\n\n        Ejemplo 2\n* Title: Pirates of the Caribbean: At World's End\n\n* Overview: Captain Barbossa, long believed to be dead, has come back to life and is headed to the edge of the Earth with Will Turner and Elizabeth Swann. But nothing is quite as it seems.\n\n* Genres: [{\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 14, \"name\": \"Fantasy\"}, {\"id\": 28, \"name\": \"Action\"}]\n\n        Ejemplo 3\n* Title: Spectre\n\n* Overview: A cryptic message from Bond’s past sends him on a trail to uncover a sinister organization. While M battles political forces to keep the secret service alive, Bond peels back the layers of deceit to reveal the terrible truth behind SPECTRE.\n\n* Genres: [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 80, \"name\": \"Crime\"}]\n\n        Ejemplo 4\n* Title: The Dark Knight Rises\n\n* Overview: Following the death of District Attorney Harvey Dent, Batman assumes responsibility for Dent's crimes to protect the late attorney's reputation and is subsequently hunted by the Gotham City Police Department. Eight years later, Batman encounters the mysterious Selina Kyle and the villainous Bane, a new terrorist leader who overwhelms Gotham's finest. The Dark Knight resurfaces to protect a city that has branded him an enemy.\n\n* Genres: [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 80, \"name\": \"Crime\"}, {\"id\": 18, \"name\": \"Drama\"}, {\"id\": 53, \"name\": \"Thriller\"}]\n\n        Ejemplo 5\n* Title: John Carter\n\n* Overview: John Carter is a war-weary, former military captain who's inexplicably transported to the mysterious and exotic planet of Barsoom (Mars) and reluctantly becomes embroiled in an epic conflict. It's a world on the brink of collapse, and Carter rediscovers his humanity when he realizes the survival of Barsoom and its people rests in his hands.\n\n* Genres: [{\"id\": 28, \"name\": \"Action\"}, {\"id\": 12, \"name\": \"Adventure\"}, {\"id\": 878, \"name\": \"Science Fiction\"}]\n\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHbbfEjR-mwz",
        "colab_type": "text"
      },
      "source": [
        "Ahora, extraeremos los tres campos del dataframe. Adicionalmente, los géneros se convierten de formato json a una lista de strings en python:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe2ZtfM2okdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# extraemos los resumenes y los guardamos en corpus\n",
        "corpus = df.overview.tolist()\n",
        "# extraemos los géneros y los guardamos en genres\n",
        "genres = list(map(lambda i: [j[\"name\"] for j in json.loads(i.replace(\"'\", \"\\\"\"))], df.genres.tolist()))\n",
        "# extraemos los títulos y los guardamos en titles\n",
        "titles = df.title\n",
        "titles"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "0                                         Avatar\n1       Pirates of the Caribbean: At World's End\n2                                        Spectre\n3                          The Dark Knight Rises\n4                                    John Carter\n                          ...                   \n4798                                 El Mariachi\n4799                                   Newlyweds\n4800                   Signed, Sealed, Delivered\n4801                            Shanghai Calling\n4802                           My Date with Drew\nName: title, Length: 4800, dtype: object"
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDWaFfEy_kVE",
        "colab_type": "text"
      },
      "source": [
        "Veamos cómo queda el dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3RRLb-26wNE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Para revisar, se imprimen los titulos en t, géneros en g y resumen en c (del resumen solo se imprimen 40 caracteres)\n",
        "for c,g,t in zip(corpus[:3], genres[:3], titles[:3]):\n",
        "    print(f\"* Título: {t}, géneros: {g}, resumen: {c[:30]}...\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPnKOe7W62xF",
        "colab_type": "text"
      },
      "source": [
        "## Preprocesamiento\n",
        "\n",
        "Utilizaremos las mismas técnicas de preprocesamiento que hemos venido aplicando a lo largo del curso. Más específicamente, convertiremos el corpus (resúmenes) a minúsculas, separaremos los signos de puntuación y eliminaremos stopwords y caracteres especiales."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xclBzA9ws2WU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus[:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ8S4VtLHDlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Función para preprocesar los textos\n",
        "def preprocessing(doc):\n",
        "    wpt = nltk.WordPunctTokenizer()\n",
        "    stop_words = nltk.corpus.stopwords.words('english')\n",
        "    # Se eliminan caracteres especiales\n",
        "    doc = re.sub('[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    # Se convierten los téxtos a minúsculas\n",
        "    doc = doc.lower()\n",
        "    # Se separan signos de puntuación\n",
        "    tokens = wpt.tokenize(doc)\n",
        "    # Se eliminan las stopwords\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "    # Retornamos una versión filtrada del texto\n",
        "    return ' '.join(tokens)\n",
        "norm_corpus = list(map(preprocessing, corpus))\n",
        "norm_corpus[:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SO0462IPG52-",
        "colab_type": "text"
      },
      "source": [
        "## Representación del texto\n",
        "\n",
        "Para representar cada documento utilizaremos **Word2Vec**: \n",
        "\n",
        "1. Este modelo se entrenará utilizando todo el corpus normalizado (resúmenes) de películas.\n",
        "2. Además, la representación de un documento será el promedio de la representación de cada uno de sus términos. \n",
        "3. Con el propósito de visualizar los resultados, utilizaremos t-SNE (técnica para reducción de dimensionalidad) para obtener vectores bidimensionales $\\vec{x_j}\\in \\mathbb{R}^2$.\n",
        "\n",
        "***Nota: una dimensión vectorial tan pequeña no es muy recomendada, aquí lo usamos sólo para visualizar el funcionamiento de métodos no supervisados***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoXL2uP3Ctr2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Se construye una representación word2vec de 100 dimensiones, \n",
        "# con una ventana de contexto de 5, usando 200 iteraciones en el modelo de aprendizaje\n",
        "\n",
        "from gensim.models import word2vec\n",
        "\n",
        "# Parámetros del modelo w2v\n",
        "feature_size = 100\n",
        "window_context = 5\n",
        "min_word_count = 1 \n",
        "sample = 1e-3\n",
        "# Definimos un tokenizador\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "# Tokenizamos el corpus\n",
        "tokenized_corpus = [wpt.tokenize(document) for document in norm_corpus]\n",
        "# Entrenamos el modelo w2v con el corpus tokenizado\n",
        "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size,\n",
        "                              window=window_context, min_count=min_word_count,\n",
        "                              sample=sample, iter=200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ddc3ShPdB7mV",
        "colab_type": "text"
      },
      "source": [
        "Recordemos que Word2Vec representa cada palabra como un vector. Para obtener una representación estructurada de un documento debemos calcular un vector promedio:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JilikkAQLCZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Se construye la representación de los documentos como el promedio de la \n",
        "# representación de cada una de sus palabras.\n",
        "def w2v_repr(corpus):\n",
        "    w2v_representations = []\n",
        "    tokenized_corpus = [wpt.tokenize(document) for document in corpus]\n",
        "    for sentence in tokenized_corpus:\n",
        "        try:\n",
        "            w2v_representations.append(w2v_model.wv[sentence].mean(axis=0))\n",
        "        except:\n",
        "            w2v_representations.append(np.zeros(shape=(feature_size,)))\n",
        "    return np.array(w2v_representations)\n",
        "\n",
        "X_w2v = w2v_repr(norm_corpus)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3W-5Wa0heDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_model.wv[[\"suspect\", \"played\"]]#.mean(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLwt-Rbft34V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X_w2v.shape)\n",
        "X_w2v[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MICK9iQPCLJw",
        "colab_type": "text"
      },
      "source": [
        "Ahora, utilizaremos t-SNE para visualizar las representaciones de Word2Vec:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDlfQZnEtu6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# se aplica técnica de reducción de dimensionalidad para visualizar en dos dimensiones \n",
        "# solo con fines de visualización\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=0, n_iter=1000, perplexity=2, verbose=1)\n",
        "T = tsne.fit_transform(X_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LCBVauAsOMMN",
        "colab": {}
      },
      "source": [
        "# Visualizamos la representación de las películas\n",
        "plt.figure(figsize=(15, 15))\n",
        "# Seleccionamos el número de documentos del que se mostrará el título\n",
        "n_docs = 50\n",
        "# Generamos una secuencia de enteros correspondiente a los índices de los documentos\n",
        "idx = np.arange(len(corpus))\n",
        "# Seleccionamos de forma aleatoria n_docs documentos\n",
        "valid_idx = np.random.choice(idx, replace=False, size=n_docs)\n",
        "# Visualizamos todas las representaciones\n",
        "plt.scatter(T[:, 0], T[:, 1], alpha=0.3)\n",
        "# Seleccionamos los títulos que se van a visualizar\n",
        "viz_titles = [titles[i] for i in valid_idx]\n",
        "# Asignamos los títulos al punto correspondiente\n",
        "for label, x, y in zip(viz_titles, T[valid_idx, 0], T[valid_idx, 1]):\n",
        "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzWysMuiNb5O",
        "colab_type": "text"
      },
      "source": [
        "# Modelamiento: **Agrupación**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1CT1YCDJRuR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## K-means\n",
        "\n",
        "Se trata del método más básico de clustering y consiste en estimar de forma automática grupos dentro de la representación de los datos, de tal forma que los puntos u objetos más parecidos queden en el mismo grupo y los grupos entre sí sean lo más diferentes posible. \n",
        "\n",
        "En la siguiente figura se muestra como el algoritmo encuentra los grupos después de varias iteraciones:\n",
        "\n",
        "![k-means](https://raw.githubusercontent.com/larajuse/share/master/kmeans.gif)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYtYlf4bvi3x",
        "colab_type": "text"
      },
      "source": [
        "En k-means clustering, tenemos que especificar el número de grupos en los que queremos que se agrupen los datos. \n",
        "\n",
        "1. El algoritmo asigna aleatoriamente cada observación a un grupo y encuentra el centroide de cada grupo. \n",
        "\n",
        "2. Luego, el algoritmo itera a través de dos pasos:\n",
        "\n",
        "    * Reasigna puntos de datos al grupo cuyo centroide es el más cercano.\n",
        "    * Calcula el nuevo centroide de cada grupo. \n",
        "  \n",
        "  Estos dos pasos se repiten hasta que la variación dentro del clúster no se pueda reducir más. La variación dentro del grupo se calcula como la suma de la distancia euclidiana entre los puntos de datos y sus respectivos centroides del grupo.\n",
        "\n",
        "Veamos un ejemplo con una versión muy simple de K-means:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHh1QkryNZwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "np.random.seed(2)\n",
        "\n",
        "# Creamos un dataset sintético\n",
        "X,_ = make_blobs(n_samples=600, centers=5, cluster_std=0.5)\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(X[:,0], X[:,1], alpha=0.4)\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$x_2$\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngnmO_C7Dbuq",
        "colab_type": "text"
      },
      "source": [
        "Definimos una función que implementa el algortmo de K-Means (esta implementación es sólo con fines demostrativos, no está optimizada ni  paralelizada)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g6n_1B8pnMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "# Definimos una función para entrenar un modelo K-Means\n",
        "def simple_kmeans(X, k, iter=100):\n",
        "    init_clus = np.random.choice(np.arange(X.shape[0]), replace=False, size=k)\n",
        "    clusters = X[init_clus]\n",
        "    preds = np.argmin(euclidean_distances(X, clusters), axis=1)\n",
        "    all_clusters = [clusters.copy()]\n",
        "    all_preds = [preds]\n",
        "    for i in range(iter):\n",
        "        for clus in range(k):\n",
        "            clusters[clus] = X[preds==clus].mean(axis=0)\n",
        "            all_clusters.append(clusters.copy())\n",
        "            preds = np.argmin(euclidean_distances(X, clusters), axis=1)\n",
        "            all_preds.append(preds.copy())\n",
        "    return all_clusters, all_preds\n",
        "\n",
        "all_clusters, all_preds = simple_kmeans(X, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkmBdjA0n7A-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iteración 0. Se realiza la inicialización aleatoriamente, para el ejemplo son 5 puntos, \n",
        "# y los demas puntos se asignan al cluster más cercano.\n",
        "\n",
        "iteration=40 #@param {type:\"slider\", min:0, max:40, step:1}\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.scatter(X[:,0], X[:,1], c=all_preds[iteration])\n",
        "plt.scatter(all_clusters[iteration][:, 0], all_clusters[iteration][:,1],c=\"r\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4hPKvguWK09",
        "colab_type": "text"
      },
      "source": [
        "Ahora veamos los grupos en la implementación de K-means en sklearn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULpyKF1LVx9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importamos el modelo de K-means\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Definimos el modelo con 5 clusters\n",
        "clf = KMeans(n_clusters=5)\n",
        "# Entrenamos el modelo\n",
        "clf.fit(X)\n",
        "# Obtenemos el cluster asignado a cada uno de los puntos\n",
        "clusters_pred = clf.predict(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDP3GhzZyF1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mostramos los resultados\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(X[:,0], X[:,1], c=clusters_pred, alpha=0.4)\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$x_2$\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hN5kNToeXWEf",
        "colab_type": "text"
      },
      "source": [
        "Podemos observar que K-means permite identificar zonas de alta densidad en el espacio de la representación. Ahora, retomaremos el caso del dataset de películas para ver qué patrones se identifican con este método. En este caso se desea **agrupar las películas**, es decir, que en cada grupo deben quedar las películas más parecidas, y los grupos entre sí deben estar lo más alejados posible. El agrupar documentos por contenido se conoce como: **Document clustering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne9pmLH4R4kO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# comenzaremos encontrando 4 clusters.\n",
        "clf = KMeans(n_clusters=4)\n",
        "clf.fit(X_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ET5OjERREv1m",
        "colab_type": "text"
      },
      "source": [
        "Ahora visualizamos los clusters asignados en dos dimensiones, con la representación obtenida de t-SNE:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLZG-xHeUXyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Obtenemos el cluster asignado a cada documento\n",
        "clusters = clf.predict(X_w2v)\n",
        "# Visualizamos las predicciones en el espacio de t-SNE\n",
        "plt.figure(figsize=(15, 15))\n",
        "# Especificamos el número de documentos del que se mostrará el título\n",
        "n_docs = 50\n",
        "# Generamos una secuencia de enteros correspondiente a los índices de los documentos\n",
        "idx = np.arange(len(corpus))\n",
        "# Seleccionamos aleatoriamente 50 documentos\n",
        "valid_idx = np.random.choice(idx, replace=False, size=n_docs)\n",
        "# Mostramos las representaciones y asignamos el color a las predicciones de K-means\n",
        "plt.scatter(T[:, 0], T[:, 1], c=clusters, alpha=0.4)\n",
        "# Seleccionamos los títulos que se van a visualizar\n",
        "viz_titles = [titles[i] for i in valid_idx]\n",
        "# Mostramos los títulos asignados a cada punto\n",
        "for label, x, y in zip(viz_titles, T[valid_idx, 0], T[valid_idx, 1]):\n",
        "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bQMaFGHWG6v",
        "colab_type": "text"
      },
      "source": [
        "Veamos algunos ejemplos de películas en cada uno de los clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBbSPsykmTPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generamos una secuencia de enteros correspondiente a los índices de los documentos\n",
        "idx = np.arange(X_w2v.shape[0])\n",
        "\n",
        "for clus in range(4):\n",
        "    # Seleccionamos aleatoriamente los índices de 10 películas dentro del cluster\n",
        "    example_idxs = np.random.choice(idx[clusters==clus], 10)\n",
        "    # Seleccionamos los títulos correspondientes\n",
        "    examples = [titles[i] for i in example_idxs]\n",
        "    print(\"Ejemplos aleatorios dentro del cluster {}\".format(clus))\n",
        "    print(examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsC5IUuSKO_o",
        "colab_type": "text"
      },
      "source": [
        "Ahora, podemos visualizar una nube de palabras con los términos más relevantes de cada cluster con el fin de interpretar qué relaciones semánticas está aprendiendo K-means:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4kJOmaUKOPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Nube de palabras\n",
        "from wordcloud import WordCloud\n",
        "# Descargamos las stopwords de NLTK\n",
        "nltk.download(\"stopwords\")\n",
        "# Importamos las stopwords\n",
        "from nltk.corpus import stopwords\n",
        "# Importamos la función reduce \n",
        "from functools import reduce\n",
        "\n",
        "# Generamos una secuencia de enteros correspondiente a los índices de los documentos\n",
        "idx = np.arange(len(norm_corpus))\n",
        "plt.figure(figsize=(20,10))\n",
        "for clus in range(4):\n",
        "    # Concatenamos las palabras de todos los documentos en cada cluster en un único string\n",
        "    # (el reduce puede ser reemplazado por \" \".join como vimos en ejemplos de clases anteriores)\n",
        "    all_words = reduce(lambda a, b:a+\" \"+b, [norm_corpus[i] for i in idx[clusters==clus]])\n",
        "    # Creamos la nube de palabras\n",
        "    im = WordCloud(stopwords=stopwords.words(\"english\")).generate(all_words)\n",
        "    # Visualizamos la nube de palabras\n",
        "    plt.subplot(221+clus)\n",
        "    plt.imshow(im, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Cluster {}\".format(clus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URTf_G2xn007",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, veamos la proporción original de géneros dentro de cada uno de los clusters encontrados. Es decir vamos a utilizar la clasificación original que tiene la película y comparar con nuestros grupos:\n",
        "\n",
        "¿Qué podemos concluir?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaCu3Fd0ny7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for clus in range(4):\n",
        "    plt.subplot(221+clus)\n",
        "    # Seleccionamos los índices correspondientes a cada cluster\n",
        "    example_idxs = idx[clusters==clus]\n",
        "    # Seleccionamos y concatenamos los géneros de todas las películas en cada cluster\n",
        "    exam_genres = reduce(lambda a, b:a+b, [genres[i] for i in example_idxs])\n",
        "    # Obtenemos un conteo de los géneros por cluster\n",
        "    names, counts = np.unique(exam_genres, return_counts=True)\n",
        "    # Normalizamos los conteos\n",
        "    counts = counts/counts.sum()\n",
        "    # Visualizamos un diagrama de pie\n",
        "    plt.pie(counts, labels=names)\n",
        "    plt.title(\"Géneros en cluster {}\".format(clus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RpVDhK0rQKg",
        "colab_type": "text"
      },
      "source": [
        "Una de las principales desventajas de K-means es que se debe escoger un número de clusters $K$. Generalmente, se selecciona utilizando información previa o por medio de un análisis exploratorio.\n",
        "\n",
        "**Método del codo (elbow)**: exploraremos cuáles son los efectos de la variación de $K$ sobre la pérdida de K-means $\\mathcal{L}$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxTo1rgJrIPu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generamos un arreglo con el número de clusters que se va a explorar\n",
        "ks = np.arange(1, 25)\n",
        "# Creamos una lista con la pérdida del modelo de K-means entrenado con el distinto número de clusters.\n",
        "scores = [-KMeans(n_clusters=k).fit(X_w2v).score(X_w2v) for k in ks]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mN2V7nwHV5f",
        "colab_type": "text"
      },
      "source": [
        "Veamos una curva de la pérdida contra el número de clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cWEw7Tjv1vU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(ks,scores,\"*-\")\n",
        "plt.xlim([ks.min(), ks.max()])\n",
        "plt.xlabel(\"$K$\")\n",
        "plt.ylabel(\"$\\mathcal{L}$\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNRH55axzQH_",
        "colab_type": "text"
      },
      "source": [
        "Esta gráfica provee información importante para la selección del número de clusters. Se trata de un *trade-off* entre un modelo más pequeño (menor número de clusters) y una pérdida menor. Lo ideal, es aprovechar el comportamiento exponencial de esta curva y seleccionar un número de clusters que reduzca de forma considerable la pérdida sin la necesidad de tener un modelo muy grande."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsPIBUKnYRUc",
        "colab_type": "text"
      },
      "source": [
        "## Affinity propagation\n",
        "\n",
        "Una alternativa a K-means es affinity propagation (AP). Se trata de un método basado en grafos que calcula de forma automática el número de clusters $K$ en un dataset.\n",
        "\n",
        "Una forma intuitiva de entender AP es suponiendo que los datos son un grafo, donde cada punto está enviando mensajes a todos los demás de forma continúa. El objetivo de estos mensajes es determinar la voluntad o complacencia de un conjunto de puntos representativos conocidos como ejemplares (análogo a los clusters en K-means). \n",
        "\n",
        "![affinity_propagation](https://www.ritchievink.com/img/post-14-affinity_propagation/preference_median.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2qwmSpXzFRC",
        "colab_type": "text"
      },
      "source": [
        "Cada punto busca determinar de forma colectiva los puntos que son representativos para cada uno por medio de dos matrices:\n",
        "\n",
        "* Una matriz de responsabilidad $\\mathbf{R}$, donde cada elemento $r_{i,k}$ representa qué tan adecuado es un punto $k$ para ser ejemplar de un punto $i$.\n",
        "* Una matriz de disponibilidad $\\mathbf{A}$, donde cada elemento $a_{i,k}$ representa qué tan apropiado sería para un punto $i$ escoger a un punto $k$ como su ejemplar.\n",
        "\n",
        "Estas dos matríces representan un grafo donde cada punto está conectado con todos los otros puntos, por ejemplo:\n",
        "\n",
        "![ap_graph](https://www.ritchievink.com/img/post-14-affinity_propagation/graph.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9H5Cpxszlxc",
        "colab_type": "text"
      },
      "source": [
        "En AP las matrices de responsabilidad y disponibilidad se modifican en base a una medida de similitud entre los datos (por ejemplo, la distancia euclidiana produce resultados similares a K-means), dicha similitud se codifica como una matríz $\\mathbf{S}$ donde cada elemento $s_{i,k}$ representa la similitud entre un punto $i$ y otro punto $k$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNrm-4Ues7DI",
        "colab_type": "text"
      },
      "source": [
        "En la práctica, affinity propagation usa los siguientes parámetros:\n",
        "\n",
        "* *affinity*: define la medida de similitud, en sklearn se puede usar \"euclidian\" o \"precomputed\". En el segundo caso se debe calcular una matríz de similitud con la distancia deseada.\n",
        "* *damping*: se trata de un parámetro usado en la implementación de affinity propagation que es análogo a una taza de aprendizaje (los autores del método sugieren valores entre 0.5 y 1). Un valor cercano a 0.5 genera un mayor número de clusters mientras un valor cercano a 1 genera un menor número de clusters.\n",
        "* *max_iter*: como se trata de un método iterativo, requiere un número máximo de iteraciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJv0tt-Hs9UM",
        "colab_type": "text"
      },
      "source": [
        "Veamos un ejemplo de la aplicación de Affinity Propagation con el mismo dataset sintético que utilizamos con K-means:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixbBoBs6AS_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importamos el modelo de Affinity Propagation\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "\n",
        "# Instanciamos el modelo\n",
        "clf = AffinityPropagation(damping=0.9)\n",
        "# Entrenamos el modelo con el dataset sintético\n",
        "clf.fit(X)\n",
        "# Obtenemos las predicciones de cada punto\n",
        "clusters_pred = clf.predict(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zGsOpbk2jrT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizamos los grupos\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.scatter(X[:,0], X[:,1], c=clusters_pred, alpha=0.4)\n",
        "plt.xlabel(\"$x_1$\")\n",
        "plt.ylabel(\"$x_2$\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opXBYMAgC0v2",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver que el método encontró automáticamente que deben haber 5 puntos ejemplares. Ahora, retomemos el ejemplo con el dataset de películas para ver cuántos clusters estima AP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEmZSBUKDL7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definimos el modelo\n",
        "clf = AffinityPropagation(damping=0.99, max_iter=1000)\n",
        "# Entrenamos el modelo con las representaciones de w2v\n",
        "clf.fit(X_w2v)\n",
        "# Obtenemos las predicciones\n",
        "clusters = clf.predict(X_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9V-YkRTIdNc",
        "colab_type": "text"
      },
      "source": [
        "Ahora, veamos cómo se ven los grupos en el espacio de t-SNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnCnFDwWDVhk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "# Definimos el número de títulos que se visualizarán\n",
        "n_docs=50\n",
        "# Generamos una secuencia de enteros con el tamaño del dataset\n",
        "idx = np.arange(len(corpus))\n",
        "# Seleccionamos los índices de los títulos que se mostrarán\n",
        "valid_idx = np.random.choice(idx, replace=False, size=n_docs)\n",
        "# Visualizamos las representaciones de t-SNE con el color asociado a la predicción\n",
        "# de affinity propagation\n",
        "plt.scatter(T[:, 0], T[:, 1], c=clusters, alpha=0.6)\n",
        "# Seleccionamos los títulos a visualizar\n",
        "viz_titles = [titles[i] for i in valid_idx]\n",
        "# Visualizamos los títulos asociados a cada punto\n",
        "for label, x, y in zip(viz_titles, T[valid_idx, 0], T[valid_idx, 1]):\n",
        "    plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVrZfQm6EGTD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Affinity Propagation encontró {} clusters\".format(clf.cluster_centers_.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHWHTfQFI5wp",
        "colab_type": "text"
      },
      "source": [
        "Ahora, veamos algunos ejemplos de películas dentro de cada cluster:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpNqeZ634HYa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generamos una secuencia de enteros correspondiente a los índices de los documentos\n",
        "idx = np.arange(X_w2v.shape[0])\n",
        "\n",
        "for clus in range(4):\n",
        "    # Seleccionamos aleatoriamente los índices de 10 películas dentro del cluster\n",
        "    example_idxs = np.random.choice(idx[clusters==clus], 10)\n",
        "    # Seleccionamos los títulos correspondientes\n",
        "    examples = [titles[i] for i in example_idxs]\n",
        "    print(\"Ejemplos aleatorios dentro del cluster {}\".format(clus))\n",
        "    print(examples)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbGY_mr-JIY_",
        "colab_type": "text"
      },
      "source": [
        "Ahora, las nubes de palabras por cluster:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4NAVTXjJQOo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generamos una secuencia de enteros correspondiente a los índices de los documentos\n",
        "idx = np.arange(len(norm_corpus))\n",
        "plt.figure(figsize=(20,10))\n",
        "for clus in range(4):\n",
        "    # Concatenamos las palabras de todos los documentos en cada cluster en un único string\n",
        "    # (el reduce puede ser reemplazado por \" \".join como vimos en ejemplos de clases anteriores)\n",
        "    all_words = reduce(lambda a, b:a+\" \"+b, [norm_corpus[i] for i in idx[clusters==clus]])\n",
        "    # Creamos la nube de palabras\n",
        "    im = WordCloud(stopwords=stopwords.words(\"english\")).generate(all_words)\n",
        "    # Visualizamos la nube de palabras\n",
        "    plt.subplot(221+clus)\n",
        "    plt.imshow(im, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Cluster {}\".format(clus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGHPGphfJxxl",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, veamos la distribución de géneros dentro de los primeros clusters:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdkwTv1W4188",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 15))\n",
        "for clus in range(8):\n",
        "    plt.subplot(421+clus)\n",
        "    # Seleccionamos los índices correspondientes a cada cluster\n",
        "    example_idxs = idx[clusters==clus]\n",
        "    # Seleccionamos y concatenamos los géneros de todas las películas en cada cluster\n",
        "    exam_genres = reduce(lambda a, b:a+b, [genres[i] for i in example_idxs])\n",
        "    # Obtenemos un conteo de los géneros por cluster\n",
        "    names, counts = np.unique(exam_genres, return_counts=True)\n",
        "    # Normalizamos los conteos\n",
        "    counts = counts/counts.sum()\n",
        "    # Visualizamos un diagrama de pie\n",
        "    plt.pie(counts, labels=names)\n",
        "    plt.title(\"Géneros en cluster {}\".format(clus))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otLeeRVEYT16",
        "colab_type": "text"
      },
      "source": [
        "## Hierarchical clustering\n",
        "\n",
        "Se trata de un algoritmo que al igual que los métodos que hemos visto, agrupa objetos similares en clusters. No obstante, se trata de un método que encuentra relaciones no lineales entre los datos al construir una estructura basada en árboles de jerarquía.\n",
        "\n",
        "![hierarchical_example](https://46gyn61z4i0t1u1pnq2bbk2e-wpengine.netdna-ssl.com/wp-content/uploads/2018/03/Screen-Shot-2018-03-28-at-11.48.48-am.png)\n",
        "\n",
        "En general, hay dos tipos de hierarchical clustering: divisivo y aglomerativo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMw2imyJrPgi",
        "colab_type": "text"
      },
      "source": [
        "### Divisive hierarchical clustering\n",
        "\n",
        "Se trata de una técnica que no es muy usada en la práctica. Utiliza el enfoque opuesto al que utiliza el método aglomerativo. Consiste en asumir que inicialmente todos los puntos pertenecen a un mismo cluster e iterativamente dividirlo en varios grupos por medio de una medida de similitud.\n",
        "\n",
        "El algoritmo más conocido que utiliza este tipo de clustering es *DIvisive ANAlysis Clustering* (DIANA), el cual estima todas las posibles formas de obtener dos particiones de cada cluster con el fin de determinar la mejor forma de dividir los grupos más heterogéneos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb-MU7bChd0a",
        "colab_type": "text"
      },
      "source": [
        "### Agglomerative hierarchical clustering\n",
        "\n",
        "El enfoque aglomerativo es el tipo más común de hierarchical clustering. Comienza suponiendo que cada uno de los puntos es un cluster, los cuales se irán fusionando a lo largo de las iteraciones hasta obtener un número deseado de clusters. El método consiste en los siguientes pasos:\n",
        "\n",
        "1. Calcular una matríz de similitud $\\mathbf{S}$.\n",
        "2. Fusionar los dos clusters más cercanos.\n",
        "3. Repetir desde el paso 1 hasta obtener un único cluster o un número $K$ de clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4wKLwbE4KLA",
        "colab_type": "text"
      },
      "source": [
        "Como un cluster puede estar compuesto por varios puntos, existen diversas formas de obtener similitudes:\n",
        "\n",
        "* MIN: la similitud entre dos clusters es la mínima similitud o distancia entre los puntos de cada cluster.\n",
        "\n",
        "![min_agglomerative](https://miro.medium.com/max/491/1*mtDL2TynaiwpJlhLdecFYQ.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rov2zfP4Ou4",
        "colab_type": "text"
      },
      "source": [
        "* MAX: la similitud entre dos clusters es la máxima distancia entre los puntos de cada cluster.\n",
        "\n",
        "![max_agglomerative](https://miro.medium.com/max/491/1*nRYZyjoT1ZRzlWp3oP0_QQ.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amtq4qB34SFl",
        "colab_type": "text"
      },
      "source": [
        "* Promedio: la similitud entre dos clusters es el promedio entre la similitud de todas las combinaciones de puntos de cada cluster.\n",
        "\n",
        "![avg_agglomerative](https://miro.medium.com/max/491/1*CMHO0wpT8hCkR_xCQW2ggQ.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61F_h-IG4V4Q",
        "colab_type": "text"
      },
      "source": [
        "* Distancia entre centroides: la similitud entre dos clusters es la similitud entre los puntos promedio o centroides de cada cluster.\n",
        "\n",
        "![centroid_agglomerative](https://miro.medium.com/max/491/1*2AYd0CXANWsM8MLwmrJzYQ.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7xiAC8R4YZU",
        "colab_type": "text"
      },
      "source": [
        "* Ward: la similitud entre dos clusters es la suma de cuadrados media de todas las combinaciones de puntos de cada cluster.\n",
        "\n",
        "Veamos la aplicación de agglomerative clustering:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqJPIT3jv-QW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importamos las librerías\n",
        "from scipy.cluster.hierarchy import ward, dendrogram\n",
        "from sklearn.metrics.pairwise import euclidean_distances"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgYlurcdw1Us",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculamos la similitud entre cada punto del dataset\n",
        "dists = euclidean_distances(X_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcYriXtKxMS3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Construímos el árbol jerárquico\n",
        "# la función ward retorna una matríz \"linkage\" con las relaciones de cada rama del árbol.\n",
        "linkage_matrix = ward(dists)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-1iIa1-3BNj",
        "colab_type": "text"
      },
      "source": [
        "El resultado es una matríz $\\mathbf{L}$ de tamaño $(N-1)\\times 4$. Cada fila $i$ contiene el resultado de cada iteración, y en las columnas se encuentra la siguiente información: $L_{i,0}$ y $L_{i,1}$ contienen los índices de los clusters que se unirán, $L_{i,2}$ contiene la similitud entre los dos clusters y $L_{i,3}$ contiene el número de puntos totales que hay en el cluster formado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds2QZ8DSxsgz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linkage_matrix.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRPcuZH46Nhu",
        "colab_type": "text"
      },
      "source": [
        "Una de las formas típicas de visualizar el resultado es con un dendograma, el cual, es una representación gráfica en forma de árbol con las relaciones determinadas. Veamos el dendograma:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9-LvBEf6jX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(10,40))\n",
        "\n",
        "# Construímos el dendograma (árbol jerárquico)\n",
        "R = dendrogram(linkage_matrix, orientation=\"left\", labels=titles,\n",
        "             truncate_mode='lastp', p=100, no_plot=False)\n",
        "\n",
        "# Definimos una función para etiquetar cada una de las ramas del árbol.\n",
        "def llf(x):\n",
        "    # Asignamos cada uno de los títulos de las películas a cada una de las ramas\n",
        "    temp = {R[\"leaves\"][i]: titles[i] for i in range(len(R[\"leaves\"]))}\n",
        "    return \"{}\".format(temp[x])\n",
        "\n",
        "# Visualizamos el dendograma\n",
        "ax = dendrogram(linkage_matrix, truncate_mode='lastp', orientation=\"left\", p=100,\n",
        "              leaf_label_func=llf, leaf_font_size=10.)\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0K78sccebg3",
        "colab_type": "text"
      },
      "source": [
        "De igual forma que en K-means, en hierarchical clustering se puede especificar un número $K$ de clusters. Así mismo, cuando no poseemos información sobre el valor de $K$ apropiado para un problema en específico, podemos realizar un análisis exploratorio.\n",
        "\n",
        "A diferencia de K-means, en este caso no se optimiza directamente una función de pérdida (la cual utilizamos para determinar un número apropiado de clusters). No obstante, existen distintas métricas que pueden ser usadas para medir el desempeño en métodos de clustering. En el caso de Hierarchical Clustering, una de las métricas más usada es la **distancia intercluster**. Consiste en obtener el promedio entre la similitud de todas las posibles combinaciones de puntos de distintos clusters. Veamos un ejemplo de distancias intercluster para un caso de $K=3$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euW1_uXeeamm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Version de Hierarchical clustering que permite seleccionar un número de clusters.\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hc = AgglomerativeClustering(n_clusters=3, linkage=\"ward\")\n",
        "hc.fit(X_w2v)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEk4mtFN_m9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Obtenemos el cluster al que pertenece cada punto\n",
        "preds = hc.labels_\n",
        "# Calculamos la distancia inter cluster\n",
        "inter_dists = np.zeros((3,3))\n",
        "for i in range(3):\n",
        "    for j in range(3):\n",
        "        if i > j: \n",
        "          inter_dists[i,j] = inter_dists[j,i]\n",
        "        else:\n",
        "            # Estimamos la distancia promedio entre el cluster i y el cluster j\n",
        "            inter_dists[i,j] = euclidean_distances(X_w2v[preds==i], X_w2v[preds==j]).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "547vo5uY_zmW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names=[\"cluster {}\".format(i) for i in range(3)]\n",
        "sns.heatmap(inter_dists,square=True,annot=True,cbar=True,\n",
        "            xticklabels=class_names,yticklabels=class_names,\n",
        "            annot_kws={\"size\": 12},fmt=\".1f\",cmap=\"viridis\",\n",
        "            vmin=0)\n",
        "plt.title(\"Distancias Intercluster Promedio\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jRrm3q4QSAU",
        "colab_type": "text"
      },
      "source": [
        "Ahora, para determinar un valor de $K$ apropiado, utilizaremos una métrica comúnmente usada en la evaluación de métodos de clustering. El Davies-Bouldin score es una razón entre las distancias intracluster y las distancias intercluster, donde un valor menor representa un mejor resultado de clustering (mayor distancia intercluster y menor distancia intracluster). Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Cj1734RA9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# En este caso utilizaremos el DB score implementado en sklearn.\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "# definimos el rango de exploración del valor k\n",
        "ks = np.arange(2,1000,50)\n",
        "scores = []\n",
        "# Iteramos para cada uno de los valores de k\n",
        "for k in ks:\n",
        "    # Entrenamos un Hierarchical clustering con cada valor de k\n",
        "    hc = AgglomerativeClustering(n_clusters=k, linkage=\"ward\")\n",
        "    hc.fit(X_w2v)\n",
        "    # Evaluamos el DB score.\n",
        "    scores.append(davies_bouldin_score(X_w2v,hc.labels_))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UF1R8d05YYlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizamos los resultados\n",
        "plt.plot(ks,scores,\"*-\")\n",
        "plt.xlim([1,ks.max()])\n",
        "plt.ylim([0,5])\n",
        "plt.xlabel(\"$K$\")\n",
        "plt.ylabel(\"Davies-Bouldin\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_L3la3p-8bKD",
        "colab_type": "text"
      },
      "source": [
        "## Ejercicio\n",
        "\n",
        "\n",
        "Repita el procedimiento mostrado en Affinity Propagation y Hierarchical clustering utilizando la similitud coseno como médida de similitud. ¿mejoran las relaciones?, discuta los resultados.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4FS2wDmGnAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3l2aCNpMR3T",
        "colab_type": "text"
      },
      "source": [
        "## Referencias\n",
        "\n",
        "* https://medium.com/@MSalnikov/text-clustering-with-k-means-and-tf-idf-f099bcf95183\n",
        "* https://www.ritchievink.com/blog/2018/05/18/algorithm-breakdown-affinity-propagation/\n",
        "* https://www.displayr.com/what-is-hierarchical-clustering/\n",
        "* https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec\n",
        "* https://www.datanovia.com/en/lessons/divisive-hierarchical-clustering/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "PLN_analisis_no_supervisado.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}